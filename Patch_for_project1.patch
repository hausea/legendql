Subject: [PATCH] Patch for project
---
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/misc.xml b/.idea/misc.xml
new file mode 100644
--- /dev/null	(date 1745251458341)
+++ b/.idea/misc.xml	(date 1745251458341)
@@ -0,0 +1,10 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="Black">
+    <option name="sdkName" value="Python 3.9 (legendql)" />
+  </component>
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.12 (legendql)" project-jdk-type="Python SDK" />
+  <component name="PythonCompatibilityInspectionAdvertiser">
+    <option name="version" value="3" />
+  </component>
+</project>
\ No newline at end of file
Index: .idea/inspectionProfiles/profiles_settings.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/inspectionProfiles/profiles_settings.xml b/.idea/inspectionProfiles/profiles_settings.xml
new file mode 100644
--- /dev/null	(date 1745251458341)
+++ b/.idea/inspectionProfiles/profiles_settings.xml	(date 1745251458341)
@@ -0,0 +1,7 @@
+<component name="InspectionProjectProfileManager">
+  <settings>
+    <option name="PROJECT_PROFILE" value="Default" />
+    <option name="USE_PROJECT_PROFILE" value="false" />
+    <version value="1.0" />
+  </settings>
+</component>
\ No newline at end of file
Index: test/simple/pure_relation_to_string_test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/simple/pure_relation_to_string_test.py b/test/simple/pure_relation_to_string_test.py
new file mode 100644
--- /dev/null	(date 1745251458346)
+++ b/test/simple/pure_relation_to_string_test.py	(date 1745251458346)
@@ -0,0 +1,218 @@
+import unittest
+
+from _datetime import datetime
+
+from dialect.purerelation.dialect import NonExecutablePureRuntime
+from model.metamodel import IntegerLiteral, InnerJoinType, BinaryExpression, ColumnAliasExpression, LiteralExpression, \
+    EqualsBinaryOperator, OperandExpression, FunctionExpression, \
+    CountFunction, AddBinaryOperator, SubtractBinaryOperator, MultiplyBinaryOperator, DivideBinaryOperator, \
+    ColumnReferenceExpression, ComputedColumnAliasExpression, MapReduceExpression, LambdaExpression, \
+    VariableAliasExpression, \
+    AverageFunction, OrderByExpression, AscendingOrderType, DescendingOrderType, IfExpression, \
+    GreaterThanBinaryOperator, DateLiteral, ModuloFunction, ExponentFunction
+from model.schema import Database, Table
+from legendql.query import Query
+
+
+class TestClauseToPureRelationDialect(unittest.TestCase):
+
+    def setUp(self):
+        pass
+
+    def test_simple_select(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .select("column")
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual("#>{local::DuckDuckDatabase.table}#->select(~[column])->from(local::DuckDuckRuntime)", pure_relation)
+
+    def test_simple_select_with_filter(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .select("column")
+                      .filter(LambdaExpression(["a"], BinaryExpression(OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("column"))), OperandExpression(LiteralExpression(IntegerLiteral(1))), EqualsBinaryOperator())))
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual("#>{local::DuckDuckDatabase.table}#->select(~[column])->filter(a | $a.column==1)->from(local::DuckDuckRuntime)", pure_relation)
+
+    def test_simple_select_with_extend(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .select("column")
+                      .extend([ComputedColumnAliasExpression("a", LambdaExpression(["a"], ColumnAliasExpression("a", ColumnReferenceExpression("column"))))])
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual("#>{local::DuckDuckDatabase.table}#->select(~[column])->extend(~[a:a | $a.column])->from(local::DuckDuckRuntime)", pure_relation)
+
+    def test_simple_select_with_groupBy(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .select("column", "column2")
+                      .group_by([ColumnReferenceExpression("column"), ColumnReferenceExpression("column2")],
+                   [ComputedColumnAliasExpression("count",
+                                                  MapReduceExpression(
+                                                      LambdaExpression(["a"], BinaryExpression(OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("column"))), OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("column2"))), AddBinaryOperator())),
+                                                      LambdaExpression(["a"], FunctionExpression(CountFunction(), [VariableAliasExpression("a")])))),
+                             ComputedColumnAliasExpression("avg",
+                                                  MapReduceExpression(
+                                                      LambdaExpression(["a"], ColumnAliasExpression("a", ColumnReferenceExpression("column"))),
+                                                      LambdaExpression(["a"], FunctionExpression(AverageFunction(), [VariableAliasExpression("a")]))))
+                    ])
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual(
+            "#>{local::DuckDuckDatabase.table}#->select(~[column, column2])->groupBy(~[column, column2], ~[count:a | $a.column+$a.column2 : a | $a->count(), avg:a | $a.column : a | $a->avg()])->from(local::DuckDuckRuntime)",
+            pure_relation)
+
+    def test_simple_select_with_limit(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .select("column")
+                      .limit(10)
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual("#>{local::DuckDuckDatabase.table}#->select(~[column])->limit(10)->from(local::DuckDuckRuntime)", pure_relation)
+
+    def test_simple_select_with_join(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .select("column")
+                      .join("local::DuckDuckDatabase", "table2", InnerJoinType(), LambdaExpression(["a", "b"], BinaryExpression(OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("column"))), OperandExpression(ColumnAliasExpression("b", ColumnReferenceExpression("column"))), EqualsBinaryOperator())))
+                      .select("column2")
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual("#>{local::DuckDuckDatabase.table}#->select(~[column])->join(#>{local::DuckDuckDatabase.table2}#, JoinKind.INNER, {a, b | $a.column==$b.column})->select(~[column2])->from(local::DuckDuckRuntime)", pure_relation)
+
+    def test_multiple_extends(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .extend([ComputedColumnAliasExpression("a", LambdaExpression(["a"], ColumnAliasExpression("a", ColumnReferenceExpression("column")))), ComputedColumnAliasExpression("b", LambdaExpression(["b"], ColumnAliasExpression("b", ColumnReferenceExpression("column"))))])
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual(
+            "#>{local::DuckDuckDatabase.table}#->extend(~[a:a | $a.column, b:b | $b.column])->from(local::DuckDuckRuntime)",
+            pure_relation)
+
+    def test_math_binary_operators(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .extend([
+                                  ComputedColumnAliasExpression("add", LambdaExpression(["a"], BinaryExpression(left=OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("column"))), right=OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("column"))), operator=AddBinaryOperator()))),
+                                  ComputedColumnAliasExpression("subtract", LambdaExpression(["a"], BinaryExpression(left=OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("column"))), right=OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("column"))), operator=SubtractBinaryOperator()))),
+                                  ComputedColumnAliasExpression("multiply", LambdaExpression(["a"], BinaryExpression(left=OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("column"))), right=OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("column"))), operator=MultiplyBinaryOperator()))),
+                                  ComputedColumnAliasExpression("divide", LambdaExpression(["a"], BinaryExpression(left=OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("column"))), right=OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("column"))), operator=DivideBinaryOperator()))),
+                              ])
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual(
+            "#>{local::DuckDuckDatabase.table}#->extend(~[add:a | $a.column+$a.column, subtract:a | $a.column-$a.column, multiply:a | $a.column*$a.column, divide:a | $a.column/$a.column])->from(local::DuckDuckRuntime)",
+            pure_relation)
+
+    def test_single_rename(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .rename(('column', 'newColumn'))
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual(
+            "#>{local::DuckDuckDatabase.table}#->rename(~column, ~newColumn)->from(local::DuckDuckRuntime)",
+            pure_relation)
+
+    def test_multiple_renames(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .rename(('columnA', 'newColumnA'), ('columnB', 'newColumnB'))
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual(
+            "#>{local::DuckDuckDatabase.table}#->rename(~columnA, ~newColumnA)->rename(~columnB, ~newColumnB)->from(local::DuckDuckRuntime)",
+            pure_relation)
+
+    def test_offset(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .offset(5)
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual(
+            "#>{local::DuckDuckDatabase.table}#->drop(5)->from(local::DuckDuckRuntime)",
+            pure_relation)
+
+    def test_order_by(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .order_by(
+                OrderByExpression(direction=AscendingOrderType(), expression=ColumnReferenceExpression(name="columnA")),
+                         OrderByExpression(direction=DescendingOrderType(), expression=ColumnReferenceExpression(name="columnB")))
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual(
+            "#>{local::DuckDuckDatabase.table}#->sort([~columnA->ascending(), ~columnB->descending()])->from(local::DuckDuckRuntime)",
+            pure_relation)
+
+    def test_conditional(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "columnA": int, "columnB": int})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .extend([ComputedColumnAliasExpression("conditional", LambdaExpression(["a"], IfExpression(test=BinaryExpression(left=OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("columnA"))), right=OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("columnB"))), operator=GreaterThanBinaryOperator()), body=ColumnAliasExpression("a", ColumnReferenceExpression("columnA")), orelse=ColumnAliasExpression("a", ColumnReferenceExpression("columnB")))))])
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual(
+            "#>{local::DuckDuckDatabase.table}#->extend(~[conditional:a | if($a.columnA>$a.columnB, | $a.columnA, | $a.columnB)])->from(local::DuckDuckRuntime)",
+            pure_relation)
+
+    def test_date(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "columnA": int, "columnB": int})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .extend([
+                        ComputedColumnAliasExpression("dateGreater", LambdaExpression(parameters=["a"], expression=BinaryExpression(left=OperandExpression(LiteralExpression(literal=DateLiteral(datetime(2025, 4, 11)))), right=OperandExpression(LiteralExpression(literal=DateLiteral(datetime(2025, 4, 12)))), operator=GreaterThanBinaryOperator()))),
+                        ComputedColumnAliasExpression("dateTimeGreater", LambdaExpression(parameters=["a"], expression=BinaryExpression(left=OperandExpression(LiteralExpression(literal=DateLiteral(datetime(2025, 4, 11, 10, 0, 0)))), right=OperandExpression(LiteralExpression(literal=DateLiteral(datetime(2025, 4, 12, 10, 0, 0)))), operator=GreaterThanBinaryOperator()))),
+                      ])
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual(
+            "#>{local::DuckDuckDatabase.table}#->extend(~[dateGreater:a | %2025-04-11T00:00:00>%2025-04-12T00:00:00, dateTimeGreater:a | %2025-04-11T10:00:00>%2025-04-12T10:00:00])->from(local::DuckDuckRuntime)",
+            pure_relation)
+
+    def test_modulo_and_exponent(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "columnA": int, "columnB": int})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .extend([
+                        ComputedColumnAliasExpression("modulo", LambdaExpression(["a"], FunctionExpression(parameters=[ColumnAliasExpression("a", ColumnReferenceExpression("column")), LiteralExpression(literal=IntegerLiteral(2))], function=ModuloFunction()))),
+                        ComputedColumnAliasExpression("exponent", LambdaExpression(["a"], FunctionExpression(parameters=[ColumnAliasExpression("a", ColumnReferenceExpression("column")), LiteralExpression(literal=IntegerLiteral(2))], function=ExponentFunction())))
+                      ])
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual(
+            "#>{local::DuckDuckDatabase.table}#->extend(~[modulo:a | $a.column->mod(2), exponent:a | $a.column->pow(2)])->from(local::DuckDuckRuntime)",
+            pure_relation)
Index: lakehouse/hcm_config.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lakehouse/hcm_config.py b/lakehouse/hcm_config.py
new file mode 100644
--- /dev/null	(date 1745251458342)
+++ b/lakehouse/hcm_config.py	(date 1745251458342)
@@ -0,0 +1,48 @@
+import lakehouse
+from lakehouse.hcm_extract import test_pandas_extract, test_polars_extract
+
+def config() -> lakehouse.Lakehouse:
+
+    lh = lakehouse.init(lakehouse.hcm_dev, deployment_id=123456, schema="people")
+
+    lh.config({
+    "metadir": lh.dataset(
+        table="metadir",
+        primary_key=["id"],
+        source=lh.pandas(test_pandas_extract),
+        versioning=lh.batch_milestone_full,
+        classification=lh.producer_only,
+        #columns={"kerberos": str, "name": str, "id": str, "department": str, "start_date": date, "nickname": Optional[str]}
+    ),
+
+    "corpdir": lh.dataset(
+        table="corpdir",
+        primary_key="kerberos",
+        source=lh.polars(test_polars_extract),
+        versioning=lh.batch_milestone_incremental,
+        classification=lh.producer_only,
+        #columns={"kerberos": str, "dept1": str, "dept2": str, "dept3": str, "start_time": datetime}
+    ),
+
+    "metadir_mv": lh.view(
+        table="metadir_mv",
+        primary_key="kerberos",
+        source=lh.query(lh.metadir)
+            .filter(lambda r: r.department == "Data Engineering")
+            .join(lh.query(lh.corpdir), lambda m, c: (m.kerberos == c.kerberos, (corp_kerberos := c.kerberos))),
+        versioning=lh.batch_milestone_full,
+        classification=lh.enterprise,
+        trigger=lh.any([lh.metadir, lh.corpdir])
+    ),
+
+    "hcm_people_dir": lh.data_product(
+        name="hcm_people_dir",
+        display_name="HCM People Directory",
+        access_points={
+            "secdb_metadir": lh.query(lh.metadir_mv),
+            "gsweb_corpdir": lh.query(lh.corpdir)
+        }
+    )
+    })
+
+    return lh
\ No newline at end of file
Index: test/simple/pure_relation_exec_server_test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/simple/pure_relation_exec_server_test.py b/test/simple/pure_relation_exec_server_test.py
new file mode 100644
--- /dev/null	(date 1745251458346)
+++ b/test/simple/pure_relation_exec_server_test.py	(date 1745251458346)
@@ -0,0 +1,30 @@
+from model.schema import Table, Database
+from legendql.query import Query
+from runtime.pure.db.duckdb import DuckDBDatabaseType
+from runtime.pure.executionserver.runtime import ExecutionServerRuntime
+from test.executionserver.test import ExecutionServerTest
+
+table = Table("employees", {"id": int, "departmentId": int, "first": str, "last": str})
+database = Database("local::DuckDuckDatabase", [table])
+
+class TestExecutionServerEvaluation(ExecutionServerTest):
+    @classmethod
+    def setUpClass(cls):
+        ExecutionServerTest.setUpClass()
+        ExecutionServerTest.create_table(table)
+        ExecutionServerTest.load_csv(table, "../data/employees.csv")
+
+    def test_execution_against_execution_server(self):
+        runtime = ExecutionServerRuntime("local::DuckDuckRuntime", DuckDBDatabaseType(ExecutionServerTest.get_duckdb_path()), "http://localhost:6300", database)
+        data_frame = (Query.from_table(database, table)
+                      .select("id", "departmentId", "first", "last")
+                      .bind(runtime))
+
+        result = data_frame.eval().data()
+
+        self.assertEqual(result.relation, "#>{local::DuckDuckDatabase.employees}#->select(~[id, departmentId, first, last])->from(local::DuckDuckRuntime)")
+        self.assertEqual(result.sql, 'select "employees_0".id as "id", "employees_0".departmentId as "departmentId", "employees_0".first as "first", "employees_0".last as "last" from employees as "employees_0"')
+        self.assertEqual(",".join(result.header), "id,departmentId,first,last")
+        self.assertEqual(len(result.rows), 2)
+        self.assertEqual(",".join(map(lambda r: str(r), result.rows[0])), "1,1, John, Doe")
+        self.assertEqual(",".join(map(lambda r: str(r), result.rows[1])), "2,1, Jane, Doe")
Index: lakehouse/hcm.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lakehouse/hcm.py b/lakehouse/hcm.py
new file mode 100644
--- /dev/null	(date 1745251458342)
+++ b/lakehouse/hcm.py	(date 1745251458342)
@@ -0,0 +1,5 @@
+import lakehouse
+import hcm_config as hcm
+
+def data_products():
+    return lakehouse.discover_data_products(hcm.config())
\ No newline at end of file
Index: test/simple/pure_relation_repl_test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/simple/pure_relation_repl_test.py b/test/simple/pure_relation_repl_test.py
new file mode 100644
--- /dev/null	(date 1745251458346)
+++ b/test/simple/pure_relation_repl_test.py	(date 1745251458346)
@@ -0,0 +1,66 @@
+import unittest
+
+from model.metamodel import BinaryExpression, OperandExpression, ColumnAliasExpression, LiteralExpression, \
+    IntegerLiteral, EqualsBinaryOperator, \
+    FunctionExpression, CountFunction, InnerJoinType, ColumnReferenceExpression, ComputedColumnAliasExpression, \
+    MapReduceExpression, LambdaExpression, VariableAliasExpression
+from model.schema import Database, Table
+from legendql.query import Query
+from runtime.pure.repl.repl_utils import is_repl_running, send_to_repl, load_csv_to_repl
+from runtime.pure.repl.runtime import ReplRuntime
+
+
+class TestReplEvaluation(unittest.TestCase):
+
+    def setUp(self):
+        if not is_repl_running():
+            self.skipTest("REPL is not running")
+        load_csv_to_repl("../data/employees.csv", "local::DuckDuckConnection", "employees")
+        load_csv_to_repl("../data/departments.csv", "local::DuckDuckConnection", "departments")
+
+    def tearDown(self):
+        send_to_repl("drop local::DuckDuckConnection employees")
+        send_to_repl("drop local::DuckDuckConnection departments")
+
+    def test_simple_select(self):
+        runtime = ReplRuntime("local::DuckDuckRuntime")
+        table = Table("employees", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .select("id", "departmentId", "first", "last")
+                      .bind(runtime))
+        results = data_frame.eval().data()
+        self.assertEqual("""> +--------+--------------+------------+------------+
+|   id   | departmentId |   first    |    last    |
+| BIGINT |    BIGINT    | VARCHAR(0) | VARCHAR(0) |
++--------+--------------+------------+------------+
+|   1    |      1       |    John    |     Doe    |
+|   2    |      1       |    Jane    |     Doe    |
++--------+--------------+------------+------------+
+2 rows -- 4 columns""", results[:results.rfind("columns") + 7])
+
+    def test_complex_query(self):
+        runtime = ReplRuntime("local::DuckDuckRuntime")
+        table = Table("employees", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (Query.from_table(database, table)
+                      .filter(LambdaExpression(["r"], BinaryExpression(OperandExpression(ColumnAliasExpression("r", ColumnReferenceExpression("departmentId"))), OperandExpression(LiteralExpression(IntegerLiteral(1))), EqualsBinaryOperator())))
+                      .select("departmentId")
+                      .extend([ComputedColumnAliasExpression("newCol", LambdaExpression(["x"], ColumnAliasExpression("x", ColumnReferenceExpression("departmentId"))))])
+                      .group_by([ColumnReferenceExpression("newCol")],
+                   [ComputedColumnAliasExpression("count",
+                                                 MapReduceExpression(
+                                                     LambdaExpression(["x"], ColumnAliasExpression("x", ColumnReferenceExpression("newCol"))),
+                                                     LambdaExpression(["x"], FunctionExpression(CountFunction(), [VariableAliasExpression("x")]))))])
+                      .limit(1)
+                      .join("local::DuckDuckDatabase", "departments", InnerJoinType(), LambdaExpression(["a", "b"], BinaryExpression(OperandExpression(ColumnAliasExpression("a", ColumnReferenceExpression("newCol"))), OperandExpression(ColumnAliasExpression("b", ColumnReferenceExpression("id"))), EqualsBinaryOperator())))
+                      .select("id")
+                      .bind(runtime))
+        results = data_frame.eval().data()
+        self.assertEqual("""> +--------+
+|   id   |
+| BIGINT |
++--------+
+|   1    |
++--------+
+1 rows -- 1 columns""", results[:results.rfind("columns") + 7])
Index: lakehouse/example_consumer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lakehouse/example_consumer.py b/lakehouse/example_consumer.py
new file mode 100644
--- /dev/null	(date 1745251458342)
+++ b/lakehouse/example_consumer.py	(date 1745251458342)
@@ -0,0 +1,36 @@
+import lakehouse
+import lakehouse.hcm as hcm
+
+def config() -> lakehouse.Lakehouse:
+
+    lh = lakehouse.init(lakehouse.gm_dev, 88888888, "positions")
+
+    lh.config({
+
+    "metadir": lh.import_(hcm.data_products(),
+        data_product="hcm_people_dir",
+        access_point="secdb_metadir",
+        import_alias="metadir"
+    ),
+
+    "firm_positions": lh.dataset(
+        table="firm_positions",
+        primary_key=["account", "product_id"],
+        columns={"dept3": str, "account": str, "product_id": int, "quantity": float},
+        source=lh.csv(file_name="/user/test/positions_2025.csv", column_delimiter="|"),
+        versioning=lh.batch_milestone_incremental,
+        classification=lh.producer_entitled
+    ),
+
+    "positions_dp": lh.data_product(
+        name="positions_dp",
+        display_name="Firm Positions",
+        access_points={
+            "positions_rle": lh.query(lh.firm_positions)
+                .join(lh.query(lh.metadir), lambda p, m: (p.dept3 == m.dept3, (metadir_dept3 := m.dept3))),
+            "positions_all": lh.query(lh.firm_positions)
+        }
+    )
+    })
+
+    return lh
\ No newline at end of file
Index: lakehouse/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lakehouse/__init__.py b/lakehouse/__init__.py
new file mode 100644
--- /dev/null	(date 1745251458342)
+++ b/lakehouse/__init__.py	(date 1745251458342)
@@ -0,0 +1,34 @@
+from __future__ import annotations
+
+from typing import Dict
+
+from lakehouse.lh import Lakehouse, DataProduct, Tenant, Development, Production
+
+
+def init(tenant: Tenant, deployment_id: int, schema: str = None) -> Lakehouse:
+    return Lakehouse(tenant, deployment_id, schema)
+
+def discover_data_products(lh: Lakehouse) -> Dict[str, DataProduct]:
+    return lh.discover_data_products()
+
+__all__ = [Lakehouse]
+
+gm_dev = Tenant("gm", Development())
+gm_prod = Tenant("gm", Production())
+ib_dev = Tenant("ib", Development())
+ib_prod = Tenant("ib", Production())
+pwm_dev = Tenant("pwm", Development())
+pwm_prod = Tenant("pwm", Production())
+am_public_dev = Tenant("awm-public", Development())
+am_public_prod = Tenant("awm-public", Production())
+am_private_dev = Tenant("am-private", Development())
+am_private_prod = Tenant("am-private", Production())
+hcm_dev = Tenant("hcm", Development())
+hcm_prod = Tenant("hcm", Production())
+cfo_dev = Tenant("cf&o", Development())
+cfo_prod = Tenant("cf&o", Production())
+eng_dev = Tenant("eng", Development())
+eng_prod = Tenant("eng", Production())
+
+
+
Index: lakehouse/lh.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lakehouse/lh.py b/lakehouse/lh.py
new file mode 100644
--- /dev/null	(date 1745251458342)
+++ b/lakehouse/lh.py	(date 1745251458342)
@@ -0,0 +1,261 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from enum import Enum
+from typing import Dict, Optional, List, Tuple, Union, Type
+
+from lakehouse.extract import Legend, IngestSource, Polars, CSV, Pandas
+from model.schema import Table
+from legendql.ql import LegendQL
+
+
+class MilestoneType(Enum):
+    batch_milestone = 'batch_milestone'
+    append_only = "append_only"
+    overwrite = "overwrite"
+
+class SnapshotType(Enum):
+    incremental = "incremental"
+    full = "full"
+    none = "none"
+
+@dataclass
+class Versioning:
+    type: Tuple[MilestoneType, SnapshotType]
+
+@dataclass
+class IngestTrigger:
+    pass
+
+@dataclass
+class Ingest(Dict):
+    datasets: Dict[str, Dataset]
+
+    def __getattr__(self, key) -> Table:
+        try:
+            return self.datasets[key].get_table_definition()
+        except KeyError as e:
+            raise AttributeError(f"'IngestConfig' object has no Dataset '{key}'") from e
+
+@dataclass
+class MaterializedView(Dict):
+    views: Dict[str, View]
+
+    def __getattr__(self, key) -> Table:
+        try:
+            return self.views[key].source.query.get_table_definition()
+        except KeyError as e:
+            raise AttributeError(f"'MaterializedView' object has no View '{key}'") from e
+
+@dataclass
+class Scheduled(IngestTrigger):
+    pass
+
+@dataclass
+class Dependency(IngestTrigger):
+    dependencies: List[Ingest]
+
+@dataclass
+class AnyDependency(Dependency):
+    pass
+
+@dataclass
+class AllDependency(Dependency):
+    pass
+
+
+@dataclass
+class Queryable(Dict):
+    def get_table_definition(self) -> Table:
+        pass
+
+@dataclass
+class AccessPoint(Queryable):
+    query: LegendQL
+    def get_table_definition(self) -> Table:
+        return self.query.get_table_definition()
+
+@dataclass
+class DataProduct(Dict):
+    name: str
+    access_points: Dict[str, AccessPoint]
+
+    def __getattr__(self, key) -> Table:
+        try:
+            return self.access_points[key].get_table_definition()
+        except KeyError as e:
+            raise AttributeError(f"'DataProduct' object has no AccessPoint '{key}'") from e
+
+@dataclass
+class Environment:
+    pass
+
+@dataclass
+class Development(Environment):
+    pass
+
+@dataclass
+class Production(Environment):
+    pass
+
+@dataclass
+class Tenant:
+    name: str
+    environment: Environment
+
+@dataclass
+class Classification:
+    level: str
+
+@dataclass
+class TableOptions:
+    pass
+
+@dataclass
+class Dataset(Queryable):
+    schema: str
+    table: str
+    primary_key: Union[str, List[str]]
+    versioning: Versioning
+    source: IngestSource
+    columns: Optional[Dict[str, Optional[Type]]] = None
+    classification: Optional[Classification] = None
+    options: Optional[TableOptions] = None
+    trigger: Optional[IngestTrigger] = None
+
+    def get_table_definition(self) -> Table:
+        if self.columns is not None:
+            return Table(self.table, self.columns)
+        else:
+            return Table(self.table, self.source.auto_infer_columns())
+
+@dataclass
+class View(Dataset):
+    source: Legend
+    def get_table_definition(self) -> Table:
+        return self.source.query.get_table_definition()
+
+dev = Development()
+prod = Production()
+
+class Lakehouse(Dict):
+    external_public = Classification("DP00")
+    enterprise = Classification("DP10")
+    producer_entitled = Classification("DP20")
+    high_risk = Classification("DP30")
+    producer_only = Classification("")
+
+    batch_milestone_full = Versioning((MilestoneType.batch_milestone, SnapshotType.incremental))
+    batch_milestone_incremental = Versioning((MilestoneType.batch_milestone, SnapshotType.full))
+    append_only = Versioning((MilestoneType.append_only, SnapshotType.incremental))
+    overwrite = Versioning((MilestoneType.overwrite, SnapshotType.none))
+
+    pandas = Pandas
+    polars = Polars
+    csv = CSV
+
+    any = AnyDependency
+    all = AllDependency
+
+    def __init__(self, tenant: Tenant, deployment_id: int, schema: str = None):
+        super().__init__()
+        self.tenant = tenant
+        self.deployment_id = deployment_id
+        self.schema = schema
+        self.configuration: Dict[str, Queryable] = {}
+        self.data_products: Dict[str, DataProduct] = {}
+        # CALL PRODUCER SETUP SCRIPT HERE!!
+
+    @classmethod
+    def query(cls, queryable: Queryable) -> LegendQL:
+        return LegendQL.from_lh(queryable.get_table_definition())
+
+    def config(self, config: Dict[str, Queryable]) -> Lakehouse:
+        #self.configuration.update(config)
+        return self
+
+    def dataset(
+            self,
+            table: str,
+            primary_key: Union[str, List[str]],
+            versioning: Versioning,
+            source: IngestSource,
+            schema: str = None,
+            columns: Dict[str, Optional[Type]] = None,
+            classification: Classification = None,
+            options: TableOptions = None,
+            trigger: IngestTrigger = None) -> Dataset:
+
+        if schema is None:
+            schema = self.schema
+
+        dataset = Dataset(schema, table, primary_key, versioning, source, columns, classification, options, trigger)
+        self.configuration[table] = dataset
+        return dataset
+
+    def run_ingest(self, dataset: Dataset):
+        # first, upload the ingest spec to the lakehouse
+        # self._upload_ingest_spec(ingest)
+        # then call first ingest API to get s3 location for upload
+        # location = self._get_staging_location()
+        # then actually do the extract and push to location
+        # self._extract_and_stage_data(location)
+        # finally, do the actual ingest
+        # self._ingest_data()
+        pass
+
+    def view(self,
+        table: str,
+        primary_key: Union[str, List[str]],
+        versioning: Versioning,
+        source: LegendQL,
+        schema: str = None,
+        columns: Dict[str, Optional[Type]] = None,
+        classification: Optional[Classification] = None,
+        options: TableOptions = None, trigger: IngestTrigger = None) -> View:
+
+        if schema is None:
+            schema = self.schema
+
+        view = View(schema, table, primary_key, versioning, Legend(source), columns, classification, options, trigger)
+        self.configuration[table] = view
+        return view
+
+    def run_matview(self, view: View):
+        # first, upload the ingest spec to the lakehouse
+        # self._upload_ingest_spec(ingest)
+        # run the materialization
+        # self._run_materialization()
+        pass
+
+    def data_product(self, name: str, display_name: str,
+                     access_points: Dict[str, LegendQL]) -> DataProduct:
+        dp = DataProduct(name, {name: AccessPoint(query) for (name, query) in access_points.items()})
+        self.data_products[name] = dp
+
+        for ap_name, query in access_points.items():
+            self.configuration[ap_name] = AccessPoint(query)
+
+        return dp
+
+    def publish_data_product(self, dp: DataProduct):
+        # dp = self.configuration[name]
+        # CALL DATA PRODUCT PUBLISH/DEPLOY API
+        pass
+
+    def discover_data_products(self) -> Dict[str, DataProduct]:
+        return self.data_products
+
+    def import_(self, dp: Dict[str, DataProduct], data_product: str, access_point: str, import_alias: str = None):
+        if import_alias is  None:
+            import_alias = access_point
+        self.configuration[import_alias] = dp[data_product].access_points[access_point]
+
+    def __getattr__(self, key: str) -> Union[Queryable, DataProduct]:
+        try:
+            return self.configuration[key]
+        except KeyError as e:
+            try:
+                return self.data_products[key]
+            except KeyError as e:
+                raise AttributeError(f"'Lakehouse' has no '{key}'") from e
Index: lakehouse/examples.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lakehouse/examples.py b/lakehouse/examples.py
new file mode 100644
--- /dev/null	(date 1745251458342)
+++ b/lakehouse/examples.py	(date 1745251458342)
@@ -0,0 +1,15 @@
+import hcm_config as hcm
+import example_consumer as gbm
+
+#PRODUCER
+lh = hcm.config()
+lh.run_ingest(lh.metadir)
+lh.run_ingest(lh.corpdir)
+lh.run_matview(lh.metadir_mv)
+lh.publish_data_product(lh.hcm_people_dir)
+
+
+# CONSUMER
+lh = gbm.config()
+lh.run_ingest(lh.firm_positions)
+lh.publish_data_product(lh.positions_dp)
\ No newline at end of file
Index: test/parser/parser_test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/parser/parser_test.py b/test/parser/parser_test.py
new file mode 100644
--- /dev/null	(date 1745251458346)
+++ b/test/parser/parser_test.py	(date 1745251458346)
@@ -0,0 +1,275 @@
+import unittest
+
+from legendql.functions import aggregate, count, over, avg, rows, unbounded
+from model.functions import StringConcatFunction, SumFunction, CountFunction, OverFunction, AvgFunction, \
+    UnboundedFunction, RowsFunction
+from model.schema import Table, Database
+from legendql.ql import LegendQL
+from legendql.parser import Parser, ParseType
+from model.metamodel import *
+
+
+class ParserTest(unittest.TestCase):
+
+    def test_select(self):
+        table = Table("employee", {"dept_id": int, "name": str})
+        database = Database("employee", [table])
+        lq = LegendQL.from_table(database, table)
+        select = lambda e: [e.dept_id, e.name]
+        p = Parser.parse(select, [lq._query._table], ParseType.select)[0]
+        self.assertEqual([ColumnReferenceExpression(name="dept_id"), ColumnReferenceExpression("name")], p)
+
+    def test_rename(self):
+        table = Table("employee", {"dept_id": int, "name": str})
+        database = Database("employee", [table])
+        lq = LegendQL.from_table(database, table)
+        rename = lambda e: [department_id := e.dept_id, full_name := e.name]
+        p = Parser.parse(rename, [lq._query._table], ParseType.rename)[0]
+        self.assertEqual([ColumnAliasExpression("department_id", ColumnReferenceExpression("dept_id")), ColumnAliasExpression("full_name", ColumnReferenceExpression("name"))], p)
+
+    def test_join(self):
+        emp = Table("employee", {"dept_id": int, "name": str})
+        dep = Table("department", {"id": int})
+        database = Database("employee", [emp, dep])
+        lq = LegendQL.from_table(database, emp)
+        jq = LegendQL.from_table(database, dep)
+        join = lambda e, d: e.dept_id == d.id
+        p = Parser.parse(join, [lq._query._table, jq._query._table], ParseType.join)[0]
+
+        self.assertEqual(LambdaExpression(["e", "d"], BinaryExpression(
+            left=OperandExpression(ColumnAliasExpression("e", ColumnReferenceExpression(name='dept_id'))),
+            right=OperandExpression(ColumnAliasExpression("d", ColumnReferenceExpression(name='id'))),
+            operator=EqualsBinaryOperator())), p)
+
+
+    def test_filter(self):
+        table = Table("employee", {"start_date": str})
+        database = Database("employee", [table])
+        lq = LegendQL.from_table(database, table)
+        filter = lambda e: e.start_date > date(2021, 1, 1)
+        p = Parser.parse(filter, [lq._query._table], ParseType.filter)[0]
+
+        self.assertEqual(LambdaExpression(["e"], BinaryExpression(
+            left=OperandExpression(ColumnAliasExpression("e", ColumnReferenceExpression(name='start_date'))),
+            right=OperandExpression(LiteralExpression(DateLiteral(date(2021, 1, 1)))),
+            operator=GreaterThanBinaryOperator())), p)  # add assertion here
+
+
+    def test_nested_filter(self):
+        table = Table("employee", {"start_date": str, "salary": str})
+        database = Database("employee", [table])
+        lq = LegendQL.from_table(database, table)
+        filter = lambda e: (e.start_date > date(2021, 1, 1)) or (e.start_date < date(2000, 2, 2)) and (e.salary < 1_000_000)
+        p = Parser.parse(filter, [lq._query._table], ParseType.filter)[0]
+
+        self.assertEqual(LambdaExpression(["e"], BinaryExpression(
+            left=OperandExpression(
+                BinaryExpression(
+                    left=OperandExpression(
+                        ColumnAliasExpression("e", ColumnReferenceExpression(name='start_date'))),
+                    right=OperandExpression(
+                        LiteralExpression(DateLiteral(date(2021, 1, 1)))),
+                    operator=GreaterThanBinaryOperator())),
+            right=OperandExpression(
+                BinaryExpression(
+                    left=OperandExpression(
+                        BinaryExpression(
+                            left=OperandExpression(
+                                ColumnAliasExpression("e", ColumnReferenceExpression(name='start_date'))),
+                            right=OperandExpression(
+                                LiteralExpression(DateLiteral(date(2000, 2, 2)))),
+                            operator=LessThanBinaryOperator())),
+                    right=OperandExpression(
+                        BinaryExpression(
+                            left=OperandExpression(
+                                ColumnAliasExpression("e", ColumnReferenceExpression(name='salary'))),
+                            right=OperandExpression(
+                                LiteralExpression(IntegerLiteral(1000000))),
+                            operator=LessThanBinaryOperator())),
+                    operator=AndBinaryOperator())),
+            operator=OrBinaryOperator())), p)
+
+
+    def test_extend(self):
+        table = Table("employee", {"salary": float, "benefits": float})
+        database = Database("employee", [table])
+        lq = LegendQL.from_table(database, table)
+        extend = lambda e: [
+            (gross_salary := e.salary + 10),
+            (gross_cost := gross_salary + e.benefits)]
+
+        p = Parser.parse(extend, [lq._query._table], ParseType.extend)[0]
+
+        self.assertEqual([
+            ComputedColumnAliasExpression(
+                alias='gross_salary',
+                expression=LambdaExpression(
+                    ["e"],
+                    BinaryExpression(
+                        left=OperandExpression(ColumnAliasExpression("e", ColumnReferenceExpression('salary'))),
+                        right=OperandExpression(LiteralExpression(IntegerLiteral(10))),
+                        operator=AddBinaryOperator()))),
+            ComputedColumnAliasExpression(
+                alias="gross_cost",
+                expression=LambdaExpression(
+                    ["e"],
+                    BinaryExpression(
+                        left=OperandExpression(ColumnAliasExpression("e", ColumnReferenceExpression("gross_salary"))),
+                        right=OperandExpression(ColumnAliasExpression("e", ColumnReferenceExpression("benefits"))),
+                        operator=AddBinaryOperator())))], p)
+
+
+    def test_sort(self):
+        table = Table("employee", {"sum_gross_cost": float, "country": str})
+        database = Database("employee", [table])
+        lq = LegendQL.from_table(database, table)
+        sort = lambda e: [+e.sum_gross_cost, -e.country]
+        p = Parser.parse(sort, [lq._query._table], ParseType.order_by)[0]
+
+        self.assertEqual( [
+            OrderByExpression(direction=AscendingOrderType(), expression=ColumnReferenceExpression(name='sum_gross_cost')),
+            OrderByExpression(direction=DescendingOrderType(), expression=ColumnReferenceExpression(name='country'))
+        ], p)
+
+
+    def test_fstring(self):
+        table = Table("employee", {"title": str, "country": str})
+        database = Database("employee", [table])
+        lq = LegendQL.from_table(database, table)
+        fstring = lambda e: (new_id := f"{e.title}_{e.country}")
+        p = Parser.parse(fstring, [lq._query._table], ParseType.extend)[0]
+
+        self.assertEqual([ComputedColumnAliasExpression(
+            alias='new_id',
+            expression=LambdaExpression(["e"], FunctionExpression(
+                function=StringConcatFunction(),
+                parameters=[
+                    ColumnAliasExpression("e", ColumnReferenceExpression(name='title')),
+                    LiteralExpression(StringLiteral("_")),
+                    ColumnAliasExpression("e", ColumnReferenceExpression(name='country'))])))], p)
+
+
+    def test_aggregate(self):
+        table = Table("employee", {"id": int, "name": str, "salary": float, "department_name": str})
+        database = Database("employee", [table])
+        lq = LegendQL.from_table(database, table)
+        group = lambda r: aggregate(
+            [r.id, r.name],
+            [sum_salary := sum(r.salary + 1), count_dept := count(r.department_name)],
+            having=sum_salary > 100_000)
+
+        #->groupBy(~[id, name], sum_salary: r | $r.salary + 1 : s | ($s)->sum()
+        p = Parser.parse(group, [lq._query._table], ParseType.group_by)[0]
+        f = GroupByExpression(
+            selections=[ColumnReferenceExpression(name="id"), ColumnReferenceExpression(name='name')],
+            expressions=[ComputedColumnAliasExpression(
+                alias='sum_salary',
+                expression=MapReduceExpression(
+                    map_expression=LambdaExpression(
+                        parameters=["r"],
+                        expression=BinaryExpression(
+                                left=OperandExpression(
+                                    ColumnAliasExpression(
+                                        alias="r",
+                                        reference=ColumnReferenceExpression("salary"))),
+                                right=OperandExpression(
+                                    LiteralExpression(
+                                        literal=IntegerLiteral(1))),
+                                operator=AddBinaryOperator())),
+                    reduce_expression=LambdaExpression(
+                        parameters=["r"],
+                        expression=FunctionExpression(
+                            function=SumFunction(),
+                            parameters=[VariableAliasExpression("r")]))
+                )),
+                ComputedColumnAliasExpression(
+                    alias='count_dept',
+                    expression=MapReduceExpression(
+                        map_expression=LambdaExpression(
+                            parameters=["r"],
+                            expression=ColumnAliasExpression("r", ColumnReferenceExpression(name='department_name'))),
+                        reduce_expression=LambdaExpression(
+                            parameters=["r"],
+                            expression=FunctionExpression(
+                                function=CountFunction(),
+                                parameters=[VariableAliasExpression("r")])))
+                )]
+            )
+
+        self.assertEqual(str(f), str(p))
+
+    @unittest.skip("need to support window")
+    def test_window(self):
+        table = Table("employee", {"location": str, "salary": float, "emp_name": str})
+        database = Database("employee", [table])
+        lq = LegendQL.from_table(database, table)
+        window = lambda r: (avg_val :=
+                            over(r.location, avg(r.salary), sort=[r.emp_name, -r.location], frame=rows(0, unbounded())))
+
+        p = Parser.parse(window, [lq._query._table], ParseType.over)[0]
+
+        f = ComputedColumnAliasExpression(
+            alias='avg_val',
+            expression=LambdaExpression(["r"], FunctionExpression(
+                function=OverFunction(),
+                parameters=[
+                    ColumnAliasExpression("r", ColumnReferenceExpression(name='location')),
+                    FunctionExpression(
+                        function=AvgFunction(),
+                        parameters=[ColumnAliasExpression("r", ColumnReferenceExpression(name='salary'))]),
+                    [ColumnAliasExpression("r", ColumnReferenceExpression(name='emp_name')),
+                     OrderByExpression(
+                         direction=DescendingOrderType(),
+                         expression=ColumnAliasExpression("r", ColumnReferenceExpression(name='location')))],
+                    FunctionExpression(
+                        function=RowsFunction(),
+                        parameters=[LiteralExpression(IntegerLiteral(0)),
+                                    FunctionExpression(function=UnboundedFunction(), parameters=[])])])))
+
+        self.assertEqual(f, p)
+
+    def test_if(self):
+        table = Table("employee", {"salary": float, "min_salary": float})
+        database = Database("employee", [table])
+        lq = LegendQL.from_table(database, table)
+        extend = lambda e: (gross_salary := e.salary if e.salary > 10 else e.min_salary)
+        p = Parser.parse(extend, [lq._query._table], ParseType.extend)[0]
+
+        self.assertEqual([ComputedColumnAliasExpression(alias='gross_salary',
+                              expression=LambdaExpression(["e"], IfExpression(test=BinaryExpression(left=OperandExpression(expression=ColumnAliasExpression("e", ColumnReferenceExpression(name='salary'))),
+                                                                            right=OperandExpression(expression=LiteralExpression(literal=IntegerLiteral(val=10))),
+                                                                            operator=GreaterThanBinaryOperator()),
+                                                      body=ColumnAliasExpression("e", ColumnReferenceExpression(name='salary')),
+                                                      orelse=ColumnAliasExpression("e", ColumnReferenceExpression(name='min_salary')))))], p);
+
+    def test_modulo(self):
+        table = Table("employee", {"salary": int})
+        database = Database("employee", [table])
+        lq = LegendQL.from_table(database, table)
+        extend = lambda e: (mod_salary := e.salary % 2 )
+        p = Parser.parse(extend, [lq._query._table], ParseType.extend)[0]
+
+        self.assertEqual([ComputedColumnAliasExpression(alias='mod_salary',
+                               expression=LambdaExpression(parameters=['e'],
+                                                           expression=FunctionExpression(function=ModuloFunction(),
+                                                                                         parameters=[ColumnAliasExpression(alias='e',
+                                                                                                                           reference=ColumnReferenceExpression(name='salary')),
+                                                                                                     LiteralExpression(literal=IntegerLiteral(val=2))])))], p);
+
+    def test_exponent(self):
+        table = Table("employee", {"salary": int})
+        database = Database("employee", [table])
+        lq = LegendQL.from_table(database, table)
+        extend = lambda e: (exp_salary := e.salary ** 2 )
+        p = Parser.parse(extend, [lq._query._table], ParseType.extend)[0]
+
+        self.assertEqual([ComputedColumnAliasExpression(alias='exp_salary',
+                               expression=LambdaExpression(parameters=['e'],
+                                                           expression=FunctionExpression(function=ExponentFunction(),
+                                                                                         parameters=[ColumnAliasExpression(alias='e',
+                                                                                                                           reference=ColumnReferenceExpression(name='salary')),
+                                                                                                     LiteralExpression(literal=IntegerLiteral(val=2))])))], p);
+
+if __name__ == '__main__':
+    unittest.main()
+
Index: test/duckdb/db.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/duckdb/db.py b/test/duckdb/db.py
new file mode 100644
--- /dev/null	(date 1745251458345)
+++ b/test/duckdb/db.py	(date 1745251458345)
@@ -0,0 +1,82 @@
+import os
+import tempfile
+from datetime import datetime, date
+
+import duckdb
+from duckdb import DuckDBPyRelation
+
+from model.schema import Table
+
+
+class DuckDB:
+    database_path: str
+
+    def __init__(self, path: str = None) -> None:
+        self.database_path = path
+
+    def get_db_path(self) -> str:
+        return str(self.database_path)
+
+    def start(self):
+        con = duckdb.connect(self.database_path)
+        con.close()
+
+    def stop(self):
+        pass
+
+    def exec_sql(self, sql: str) -> DuckDBPyRelation:
+        try:
+            con = duckdb.connect(self.database_path)
+            return con.sql(sql)
+        finally:
+            if 'con' in locals():
+                con.close()
+
+    def load_csv(self, table: Table , csv_path: str):
+        self.exec_sql(f"INSERT INTO {table.table} SELECT * FROM read_csv('{csv_path}');")
+
+    def create_table(self, table: Table):
+        columns = []
+        for (col, typ) in table.columns.items():
+            columns.append(f"{col} {self._to_column_type(typ)}")
+        create = f"CREATE TABLE {table.table} ({", ".join(columns)});"
+        self.exec_sql(create)
+
+    def drop_table(self, table: Table):
+        self.exec_sql(f"DROP TABLE {table.table};")
+
+    def _to_column_type(self, typ):
+        if typ == str:
+            return "VARCHAR(0)"
+        if typ == int:
+            return "BIGINT"
+        if typ == date:
+            return "DATE"
+        raise TypeError(f"Unkonwn column type {typ}")
+
+class TestDuckDB:
+    database_path: tempfile.NamedTemporaryFile
+    db: DuckDB
+
+    def start(self):
+        file = tempfile.NamedTemporaryFile(mode="w+", delete_on_close=True)
+        self.database_path = file.name
+        file.close()
+        self.db = DuckDB(self.database_path)
+        self.db.start()
+
+    def stop(self):
+        os.remove(self.database_path)
+        self.db.stop()
+
+def main():
+    duckdb = DuckDB("/Users/ahauser/Downloads/duckdb")
+    duckdb.start()
+    table = Table("employees", {"id": int, "departmentId": int, "first": str, "last": str})
+    print(duckdb.create_table(table))
+    duckdb.load_csv(table, "../data/employees.csv")
+    print(duckdb.drop_table(table))
+    duckdb.stop()
+
+if __name__ == "__main__":
+    main()
\ No newline at end of file
Index: lakehouse/test_extract.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lakehouse/test_extract.py b/lakehouse/test_extract.py
new file mode 100644
--- /dev/null	(date 1745251458342)
+++ b/lakehouse/test_extract.py	(date 1745251458342)
@@ -0,0 +1,188 @@
+import unittest
+import os
+import pandas as pd
+import pyarrow as pa
+import fastavro
+
+from extract import (
+    CSV, Parquet, Json, Pandas, Polars, DuckDb,
+    NumPy, Arrow, Avro, Excel
+)
+from hcm_extract import (
+    test_pandas_extract, test_polars_extract,
+    test_numpy_extract, test_arrow_table,
+    test_duckdb_extract
+)
+
+
+class TestIngestSources(unittest.TestCase):
+    @classmethod
+    def setUpClass(cls):
+        # Create test files for file-based sources
+        # CSV
+        pd.DataFrame({
+            'name': ['Alice', 'Bob'],
+            'age': [25, 30]
+        }).to_csv('test.csv', index=False)
+
+        # Parquet
+        pd.DataFrame({
+            'id': [1, 2],
+            'value': ['a', 'b']
+        }).to_parquet('test.parquet')
+
+        # JSON
+        pd.DataFrame({
+            'name': ['Charlie', 'David'],
+            'score': [90.5, 85.0]
+        }).to_json('test.json', orient='records')
+
+        # Excel - Create test file with multiple sheets
+        with pd.ExcelWriter('test.xlsx', engine='openpyxl') as writer:
+            # First sheet (default)
+            pd.DataFrame({
+                'employee': ['John', 'Jane'],
+                'salary': [50000, 60000],
+                'department': ['IT', 'HR']
+            }).to_excel(writer, sheet_name='Sheet1', index=False)
+
+            # Second sheet
+            pd.DataFrame({
+                'department': ['IT', 'HR', 'Finance'],
+                'budget': [100000, 80000, 120000]
+            }).to_excel(writer, sheet_name='Departments', index=False)
+
+        # Avro - Create test file with fastavro
+        schema = {
+            'name': 'test_record',
+            'type': 'record',
+            'fields': [
+                {'name': 'name', 'type': 'string'},
+                {'name': 'value', 'type': 'int'},
+                {'name': 'department', 'type': 'string'}
+            ]
+        }
+
+        records = [
+            {'name': 'test', 'value': 42, 'department': 'IT'},
+            {'name': 'test2', 'value': 43, 'department': 'HR'}
+        ]
+
+        with open('test.avro', 'wb') as out:
+            fastavro.writer(out, schema, records)
+
+    def test_excel_columns_no_sheet_specified(self):
+        """Test Excel source with no sheet specified (should read first sheet)"""
+        excel_source = Excel(file_name='test.xlsx')
+        schema = excel_source.auto_infer_columns()
+
+        # Check if all expected columns from first sheet are present
+        expected_columns = {'employee', 'salary', 'department'}
+        self.assertEqual(set(schema.keys()), expected_columns)
+
+        # Verify column types
+        self.assertTrue(isinstance(schema['employee'], pa.DataType))
+        self.assertTrue(isinstance(schema['salary'], pa.DataType))
+        self.assertTrue(isinstance(schema['department'], pa.DataType))
+
+    def test_excel_columns_explicit_sheet(self):
+        """Test Excel source with explicitly specified sheet"""
+        excel_source = Excel(file_name='test.xlsx', sheet_name='Departments')
+        schema = excel_source.auto_infer_columns()
+
+        # Check if all expected columns are present
+        expected_columns = {'department', 'budget'}
+        self.assertEqual(set(schema.keys()), expected_columns)
+
+        # Verify column types
+        self.assertTrue(isinstance(schema['department'], pa.DataType))
+        self.assertTrue(isinstance(schema['budget'], pa.DataType))
+
+    def test_excel_invalid_sheet(self):
+        """Test Excel source with invalid sheet name"""
+        excel_source = Excel(file_name='test.xlsx', sheet_name='NonexistentSheet')
+        with self.assertRaises(Exception):
+            schema = excel_source.auto_infer_columns()
+
+    def test_avro_columns(self):
+        """Test Avro source schema extraction"""
+        avro_source = Avro(file_name='test.avro')
+        schema = avro_source.auto_infer_columns()
+
+        # Check if all expected columns are present
+        expected_columns = {'name', 'value', 'department'}
+        self.assertEqual(set(schema.keys()), expected_columns)
+
+        # Verify column types
+        self.assertTrue(isinstance(schema['name'], pa.DataType))
+        self.assertTrue(isinstance(schema['value'], pa.DataType))
+        self.assertTrue(isinstance(schema['department'], pa.DataType))
+
+        # Test specific type mappings
+        self.assertEqual(str(schema['name']), 'string')
+        self.assertEqual(str(schema['value']), 'int32')
+        self.assertEqual(str(schema['department']), 'string')
+
+    def test_csv_columns(self):
+        csv_source = CSV(file_name='test.csv')
+        schema = csv_source.auto_infer_columns()
+        self.assertIn('name', schema)
+        self.assertIn('age', schema)
+
+    def test_parquet_columns(self):
+        parquet_source = Parquet(file_name='test.parquet')
+        schema = parquet_source.auto_infer_columns()
+        self.assertIn('id', schema)
+        self.assertIn('value', schema)
+
+    def test_json_columns(self):
+        json_source = Json(file_name='test.json')
+        schema = json_source.auto_infer_columns()
+        self.assertIn('name', schema)
+        self.assertIn('score', schema)
+
+    def test_pandas_columns(self):
+        pandas_source = Pandas(func_or_df=test_pandas_extract)
+        schema = pandas_source.auto_infer_columns()
+        expected_columns = ['kerberos', 'name', 'department', 'start_date', 'nickname']
+        for col in expected_columns:
+            self.assertIn(col, schema)
+
+    def test_polars_columns(self):
+        polars_source = Polars(func_or_df=test_polars_extract)
+        schema = polars_source.auto_infer_columns()
+        expected_columns = ['name', 'age', 'city']
+        for col in expected_columns:
+            self.assertIn(col, schema)
+
+    def test_numpy_columns(self):
+        numpy_source = NumPy(func_or_df=test_numpy_extract)
+        schema = numpy_source.auto_infer_columns()
+        self.assertTrue(len(schema) > 0)  # NumPy arrays will have at least one column
+
+    def test_arrow_columns(self):
+        arrow_source = Arrow(func_or_df=test_arrow_table)
+        schema = arrow_source.auto_infer_columns()
+        self.assertIn('a', schema)
+
+    def test_duckdb_columns(self):
+        duckdb_source = DuckDb(func_or_df=test_duckdb_extract)
+        schema = duckdb_source.auto_infer_columns()
+        expected_columns = ['id', 'kerberos']
+        for col in expected_columns:
+            self.assertIn(col, schema)
+
+    @classmethod
+    def tearDownClass(cls):
+        # Clean up test files
+        test_files = [
+            'test.csv', 'test.parquet', 'test.json',
+            'test.xlsx', 'test.avro'
+        ]
+        for file in test_files:
+            if os.path.exists(file):
+                os.remove(file)
+
+
+if __name__ == '__main__':
+    unittest.main()
\ No newline at end of file
Index: lakehouse/extract.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lakehouse/extract.py b/lakehouse/extract.py
new file mode 100644
--- /dev/null	(date 1745251458342)
+++ b/lakehouse/extract.py	(date 1745251458342)
@@ -0,0 +1,149 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Callable, Dict, Optional, Type, Union
+
+import duckdb
+import numpy as np
+import pandas as pd
+import polars as pl
+import pyarrow as pa
+import pyarrow.dataset as ds
+
+from legendql.ql import LegendQL
+
+
+@dataclass
+class IngestSource:
+    def auto_infer_columns(self) -> Dict[str, Optional[Type]]:
+        pass
+
+
+@dataclass
+class FileSource(IngestSource):
+    file_name: str
+
+
+@dataclass
+class CSV(FileSource):
+    column_delimiter: str = ',',
+    row_delimiter: str = '\n',
+    quote_character: str = '"'
+    escape_character: str = '\\'
+    starting_row = 0
+
+    def auto_infer_columns(self) -> Dict[str, Optional[Type]]:
+        table = ds.dataset(self.file_name, format="csv")
+        return dict(zip(table.schema.names, table.schema.types))
+
+
+@dataclass
+class Avro(FileSource):
+    def auto_infer_columns(self) -> Dict[str, Optional[Type]]:
+        # Use DuckDB to read Avro file and convert to Arrow table
+        con = duckdb.connect()
+        relation = con.sql(f"SELECT * FROM read_avro('{self.file_name}')")
+        table = relation.arrow()
+        return dict(zip(table.schema.names, table.schema.types))
+
+
+@dataclass
+class Parquet(FileSource):
+    def auto_infer_columns(self) -> Dict[str, Optional[Type]]:
+        table = ds.dataset(self.file_name, format="parquet")
+        return dict(zip(table.schema.names, table.schema.types))
+
+
+@dataclass
+class Json(FileSource):
+    def auto_infer_columns(self) -> Dict[str, Optional[Type]]:
+        # Read the JSON file using DuckDB instead of direct PyArrow JSON reader
+        con = duckdb.connect()
+        relation = con.sql(f"SELECT * FROM read_json('{self.file_name}', auto_detect=true)")
+        table = relation.arrow()
+        return dict(zip(table.schema.names, table.schema.types))
+
+
+@dataclass
+class Excel(FileSource):
+    sheet_name: Optional[str] = None  # Made optional
+
+    def auto_infer_columns(self) -> Dict[str, Optional[Type]]:
+        # Use DuckDB to read Excel file and convert to Arrow table
+        con = duckdb.connect()
+
+        # Build the SQL query based on whether sheet_name is provided
+        if self.sheet_name:
+            query = f"SELECT * FROM read_xlsx('{self.file_name}', sheet='{self.sheet_name}')"
+        else:
+            query = f"SELECT * FROM read_xlsx('{self.file_name}')"
+
+        relation = con.sql(query)
+        table = relation.arrow()
+        return dict(zip(table.schema.names, table.schema.types))
+
+
+@dataclass
+class PythonSource(IngestSource):
+    func_or_df: Union[Callable]
+
+    def auto_infer_columns(self) -> Dict[str, Optional[Type]]:
+        df = self.func_or_df
+
+        if isinstance(self.func_or_df, Callable):
+            df = self.func_or_df()
+
+        table = self.get_arrow_table(df)
+        return dict(zip(table.schema.names, table.schema.types))
+
+    def get_arrow_table(self, df):
+        return pa.table(df)
+
+
+@dataclass
+class Pandas(PythonSource):
+    func_or_df: Union[Callable[[], pd.DataFrame]]
+
+
+@dataclass
+class Polars(PythonSource):
+    func_or_df: Union[Callable[[], pl.DataFrame]]
+
+
+@dataclass
+class DuckDb(PythonSource):
+    func_or_df: Union[Callable[[], duckdb.DuckDBPyRelation]]
+
+
+@dataclass
+class NumPy(PythonSource):
+    func_or_df: Union[Callable[[], np.ndarray]]
+
+    def get_arrow_table(self, df):
+        return pa.table(pl.from_numpy(df))
+
+
+@dataclass
+class Arrow(PythonSource):
+    func_or_df: Union[Callable[[], pa.Table]]
+
+    def get_arrow_table(self, df):
+        return df
+
+
+@dataclass
+class Legend(IngestSource):
+    query: LegendQL
+
+# def upload_extract(ingest: Ingest, source: PythonSource):
+#     # # call Lakehouse Ingest API to get S3 location
+#     # ingest_stage_location = "/Users/neema/"
+#     #
+#     # #create file name
+#     # for dataset in ingest.datasets:
+#     #     # run the python code to get the dataframe
+#     #     df = dataset.source.func()
+#     #
+#     #     #use duckdb to write the file to s3
+#     #     duckdb.sql(f"COPY (SELECT * FROM df) TO '{ingest_stage_location}'")
+#     pass
\ No newline at end of file
Index: test/executionserver/userTestConfig.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/executionserver/userTestConfig.json b/test/executionserver/userTestConfig.json
new file mode 100644
--- /dev/null	(date 1745251458345)
+++ b/test/executionserver/userTestConfig.json	(date 1745251458345)
@@ -0,0 +1,84 @@
+{
+  "deployment": {
+    "mode": "TEST_IGNORE_FUNCTION_MATCH"
+  },
+  "logging": {
+    "level": "error",
+    "appenders": [
+      {
+        "type": "console",
+        "logFormat": "%msg\r\n"
+      }
+    ]
+  },
+  "pac4j": {
+    "bypassPaths": [
+      "/api/server/v1/info"
+    ],
+    "clients": [
+      {
+        "org.pac4j.core.client.direct.AnonymousClient": {
+        }
+      }
+    ],
+    "mongoSession": {
+      "enabled": false
+    }
+  },
+  "opentracing": {
+    "elastic": "",
+    "zipkin": "",
+    "uri": "",
+    "authenticator": {
+      "principal": "",
+      "keytab": ""
+    }
+  },
+  "swagger": {
+    "title": "Legend Engine",
+    "resourcePackage": "org.finos.legend",
+    "uriPrefix": "/api"
+  },
+  "sessionCookie": "LEGEND_ENGINE_JSESSIONID",
+  "server": {
+    "type": "simple",
+    "applicationContextPath": "/",
+    "adminContextPath": "/admin",
+    "connector": {
+      "maxRequestHeaderSize": "32KiB",
+      "type": "http",
+      "port": 6300
+    },
+    "requestLog": {
+      "appenders": [
+      ]
+    }
+  },
+  "metadataserver": {
+    "pure": {
+      "host": "127.0.0.1",
+      "port": 8080
+    },
+    "alloy" : {
+      "host": "127.0.0.1",
+      "port": 6200,
+      "prefix": "/depot/api"
+    },
+    "sdlc": {
+      "host": "localhost",
+      "port": 6100
+    }
+  },
+  "temporarytestdb": {
+    "port": 9092
+  },
+  "relationalexecution": {
+    "tempPath": "/tmp/",
+    "temporarytestdb": {
+      "port": 9092
+    }
+  },
+  "errorhandlingconfiguration": {
+    "enabled": true
+  }
+}
Index: model/schema.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/model/schema.py b/model/schema.py
new file mode 100644
--- /dev/null	(date 1745251458343)
+++ b/model/schema.py	(date 1745251458343)
@@ -0,0 +1,17 @@
+from dataclasses import dataclass
+from typing import Dict, Type, Optional, List
+
+
+@dataclass
+class Table:
+    table: str
+    columns: Dict[str, Optional[Type]]
+
+    def validate_column(self, column: str) -> bool:
+        return column in self.columns
+
+@dataclass
+class Database:
+    name: str
+    tables: List[Table]
+    pass
Index: lakehouse/hcm_extract.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lakehouse/hcm_extract.py b/lakehouse/hcm_extract.py
new file mode 100644
--- /dev/null	(date 1745251458342)
+++ b/lakehouse/hcm_extract.py	(date 1745251458342)
@@ -0,0 +1,102 @@
+from datetime import date
+
+import duckdb
+import numpy as np
+import pandas as pd
+import polars as pl
+import pyarrow as pa
+from numpy import ndarray
+
+
+def test_polars_extract() -> pd.DataFrame:
+    data = {'name': ['Alice', 'Bob', 'Charlie'],
+            'age': [25, 30, 22],
+            'city': ['New York', 'London', 'Paris']}
+    return pl.DataFrame(data)
+
+def test_pandas_extract() -> pd.DataFrame:
+    data = {
+        "id": [1, 2, 3, 4, 5],
+        "kerberos": ["nmr", "pdb", "dxl", "sc", "rr"],
+        "name": ["neema", "pierre", "david", "suraj", "ram"],
+        "department": ["data", "data", "data", "data", "data"],
+        "start_date": [date(2003, 1, 1), date(2003, 1, 1), date(2003, 1, 1), date(2003, 1, 1), date(2003, 1, 1)],
+        "nickname": [ "neems", "pi", None, "sur", "r" ]
+    }
+
+    df = pd.DataFrame(data)
+    df.set_index("id", inplace=True)
+    return df
+
+def test_numpy_extract() -> ndarray:
+    return np.array([(1, 9.0), (2, 8.0), (3, 7.0)])
+
+def test_arrow_table() -> pa.Table:
+    return pa.Table.from_pydict({'a': [42]})
+
+def test_duckdb_extract() -> duckdb.DuckDBPyRelation:
+    return duckdb.sql("SELECT 42 as id, 'neema' as kerberos")
+
+
+# df = test_pandas_extract()
+# print(df.attrs)
+# table = pa.Table.from_pandas(df)
+# print(table.schema)
+# d = dict(zip(table.schema.names, table.schema.types))
+# print(d)
+#
+# df = test_polars_extract()
+# print(pa.table(df).schema)
+#
+# df = test_numpy_extract()
+# print(df)
+# print(pa.Table.from_arrays(df))
+
+# print(pa.table(pl.from_numpy(df)))
+#
+# df = test_duckdb_extract()
+# print(pa.table(df).schema)
+#
+# df = test_arrow_table()
+# print(df.schema)
+
+
+# duckdb.sql("COPY (FROM generate_series(100_000)) TO 'test.parquet' (FORMAT parquet)")
+
+# import pyarrow.parquet as pq
+# table = pq.read_table('test.parquet')
+
+# import pyarrow.dataset as ds
+# d = ds.dataset('test.parquet', format="parquet")
+# print("parquet")
+# print(dict(zip(d.schema.names, d.schema.types)))
+
+# dd = duckdb.sql("SELECT * FROM df").pl()
+# # print(dict(zip(dd.columns, dd.dtypes)))
+# print(dd.schema)
+# print(dd)
+
+
+
+# df = test_polars_extract()
+# print(df.schema)
+#
+# dd = duckdb.sql("SELECT * FROM df")
+# #print(dict(zip(dd.columns, dd.dtypes)))
+# duckdb.sql("COPY (SELECT * FROM df) TO '/Users/neema/neema_test.csv'")
+#
+#
+# pd_df = test_pandas_extract()
+# dd = duckdb.sql("SELECT * FROM pd_df")
+# print(type(dd))
+# # <class 'duckdb.duckdb.DuckDBPyRelation'>
+#
+# print(dict(zip(dd.columns, dd.dtypes)))
+#
+# arr = test_arrow_table()
+# arrd = duckdb.sql("SELECT * FROM arr")
+# print(arrd)
+#
+# npy = test_numpy_extract()
+# npyd = duckdb.sql("SELECT * FROM npy")
+# print(npyd)
\ No newline at end of file
Index: test/executionserver/testutils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/executionserver/testutils.py b/test/executionserver/testutils.py
new file mode 100644
--- /dev/null	(date 1745251458345)
+++ b/test/executionserver/testutils.py	(date 1745251458345)
@@ -0,0 +1,54 @@
+import subprocess
+import threading
+import queue
+import time
+
+
+class TestExecutionServer:
+    path: str
+    process = None
+    repl_output_queue = queue.Queue()
+    ready = threading.Event()
+
+    def __init__(self, path):
+        self.path = path
+
+    def start(self):
+        if self.process is not None:
+            print("Execution server already running")
+            return
+
+        cmd = "java -jar " + self.path + "/legend-engine-server-http-server-4.78.3-shaded.jar server " + self.path + "/userTestConfig.json"
+
+        self.process = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)
+        def wait_for_running():
+            while self.process.poll() is None:
+                line = self.process.stdout.readline()
+                if line:
+                    if "URL_FACTORY_REGISTERED" in line:
+                        ## need to better read logs to determine if it has started
+                        time.sleep(30)
+                        self.ready.set()
+
+        wait_thread = threading.Thread(target=wait_for_running, daemon=True)
+        wait_thread.start()
+
+        if not self.ready.wait(timeout=45):
+            raise RuntimeError("Execution server did not respond")
+
+
+    def stop(self):
+        if self.process is not None:
+            self.process.terminate()
+
+
+
+def main():
+    exec_server = TestExecutionServer(".")
+    exec_server.start()
+    print("Started!")
+    time.sleep(60)
+    exec_server.stop()
+
+if __name__ == "__main__":
+    main()
\ No newline at end of file
Index: model/functions.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/model/functions.py b/model/functions.py
new file mode 100644
--- /dev/null	(date 1745251458343)
+++ b/model/functions.py	(date 1745251458343)
@@ -0,0 +1,92 @@
+from dataclasses import dataclass
+
+from model.metamodel import Function, ExecutionVisitor
+
+
+@dataclass
+class AggregationFunction(Function):
+    # sum(), min(), max(), count(), avg()
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class ScalarFunction(Function):
+    # date_diff(), left(), abs() ..
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class WindowFunction(Function):
+    # rank(), row_number(), first(), last() ..
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class RankFunction(WindowFunction):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class RowNumberFunction(WindowFunction):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class LeadFunction(WindowFunction):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class LagFunction(WindowFunction):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class LeftFunction(ScalarFunction):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class StringConcatFunction(ScalarFunction):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class AvgFunction(AggregationFunction):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class CountFunction(AggregationFunction):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class SumFunction(AggregationFunction):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class OverFunction(ScalarFunction):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class RowsFunction(ScalarFunction):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class RangeFunction(ScalarFunction):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class UnboundedFunction(ScalarFunction):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
+
+@dataclass
+class AggregateFunction(ScalarFunction):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        raise NotImplementedError()
Index: legendql/functions.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/legendql/functions.py b/legendql/functions.py
new file mode 100644
--- /dev/null	(date 1745251458342)
+++ b/legendql/functions.py	(date 1745251458342)
@@ -0,0 +1,98 @@
+import ast
+from abc import ABC
+from dataclasses import dataclass
+from typing import Union, Optional
+
+
+class AggregationFunction:
+    pass
+
+class AvgFunction(AggregationFunction, float):
+    pass
+
+class CountFunction(AggregationFunction, int):
+    pass
+
+class SumFunction(AggregationFunction, float):
+    pass
+
+class LeftFunction(str):
+    def __init__(self, expr: ast.Expr, count_: int):
+        pass
+
+class WindowFunction:
+    pass
+
+class RankFunction(WindowFunction, int):
+    pass
+
+class RowNumberFunction(WindowFunction, int):
+    pass
+
+class LeadFunction(WindowFunction, int):
+    def __init__(self,
+        expression: ast.Expr,
+        offset: Optional[int] = 1,
+        default: Optional[ast.Expr] = None):
+        pass
+
+class LagFunction(WindowFunction, int):
+    def __init__(self,
+        expression: ast.Expr,
+        offset: Optional[int] = 1,
+        default: Optional[ast.Expr] = None):
+        pass
+
+@dataclass
+class AggregateFunction:
+    def __init__(self,
+        columns: Union[ast.expr, list[ast.expr]],
+        functions: Union[AggregationFunction, list[AggregationFunction]],
+        having: Optional[bool] = None):
+        pass
+
+@dataclass
+class UnboundedFunction:
+    def __init__(self):
+        pass
+
+
+@dataclass
+class Frame(ABC):
+    pass
+
+
+@dataclass
+class RowsFunction(Frame):
+    def __init__(self, start: Union[int, UnboundedFunction], end: Union[int, UnboundedFunction]):
+        super().__init__(start, end)
+
+
+@dataclass
+class RangeFunction(Frame):
+    def __init__(self, start: Union[int, UnboundedFunction], end: Union[int, UnboundedFunction]):
+        super().__init__(start, end)
+
+@dataclass
+class OverFunction:
+    def __init__(self,
+        columns: Union[ast.expr, list[ast.expr]],
+        functions: Union[AggregationFunction, WindowFunction, list[AggregationFunction], list[WindowFunction]],
+        sort: Optional[Union[ast.expr, list[ast.expr]]] = None,
+        frame: Optional[Frame] = None,
+        qualify: Optional[bool] = None):
+        pass
+
+aggregate = AggregateFunction
+over = OverFunction
+unbounded = UnboundedFunction
+rows = RowsFunction
+range = RangeFunction
+left = LeftFunction
+avg = AvgFunction
+count = CountFunction
+sum = SumFunction
+rank = RankFunction
+lead = LeadFunction
+lag = LagFunction
+row_number = RowNumberFunction
Index: legendql/query.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/legendql/query.py b/legendql/query.py
new file mode 100644
--- /dev/null	(date 1745251458343)
+++ b/legendql/query.py	(date 1745251458343)
@@ -0,0 +1,75 @@
+from __future__ import annotations
+from dataclasses import dataclass
+from typing import List, Tuple, Type, Dict
+
+from model.metamodel import SelectionClause, Runtime, DataFrame, FilterClause, ExtendClause, GroupByClause, \
+    LimitClause, JoinClause, JoinType, JoinExpression, Clause, FromClause, Expression, IntegerLiteral, \
+    GroupByExpression, ColumnReferenceExpression, RenameClause, ColumnAliasExpression, OffsetClause, OrderByExpression, \
+    OrderByClause
+from model.schema import Table, Database
+
+
+@dataclass
+class Query:
+    _database: Database
+    _table_history: []
+    _table: Table
+    _clauses: List[Clause]
+
+    @classmethod
+    def from_table(cls, database: Database, table: Table) -> Query:
+        table = Table(table.table, table.columns.copy())
+        return Query(database, [table], table, [FromClause(database.name, table.table)])
+
+    @classmethod
+    def from_db(cls, database: Database, table: str, columns: Dict[str, Type]) -> Query:
+        return Query.from_table(database, Table(table, columns))
+
+    def bind[R: Runtime](self, runtime: R) -> DataFrame:
+        return DataFrame(runtime, self._clauses)
+
+    def eval[R: Runtime, T](self, runtime: R) -> DataFrame:
+        return self.bind(runtime).eval()
+
+    def _add_clause(self, clause: Clause) -> None:
+        self._clauses.append(clause)
+
+    def _update_table(self, table: Table) -> None:
+        self._table_history.append(table)
+        self._table = table
+
+    def select(self, *names: str) -> Query:
+        self._add_clause(SelectionClause(list(map(lambda name: ColumnReferenceExpression(name), names))))
+        return self
+
+    def rename(self, *renames: Tuple[str, str]) -> Query:
+        self._add_clause(RenameClause(list(map(lambda rename: ColumnAliasExpression(alias=rename[1], reference=ColumnReferenceExpression(name=rename[0])), renames))))
+        return self
+
+    def extend(self, extend: List[Expression]) -> Query:
+        self._add_clause(ExtendClause(extend))
+        return self
+
+    def filter(self, filter_clause: Expression) -> Query:
+        self._add_clause(FilterClause(filter_clause))
+        return self
+
+    def group_by(self, selections: List[Expression], group_by: List[Expression], having: Expression = None) -> Query:
+        self._add_clause(GroupByClause(GroupByExpression(selections, group_by, having)))
+        return self
+
+    def limit(self, limit: int) -> Query:
+        self._add_clause(LimitClause(IntegerLiteral(limit)))
+        return self
+
+    def offset(self, offset: int) -> Query:
+        self._add_clause(OffsetClause(IntegerLiteral(offset)))
+        return self
+
+    def order_by(self, *ordering: OrderByExpression) -> Query:
+        self._add_clause(OrderByClause(list(ordering)))
+        return self
+
+    def join(self, database: str, table: str, join_type: JoinType, on_clause: Expression) -> Query:
+        self._add_clause(JoinClause(FromClause(database, table), join_type, JoinExpression(on_clause)))
+        return self
Index: test/executionserver/test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/executionserver/test.py b/test/executionserver/test.py
new file mode 100644
--- /dev/null	(date 1745251458345)
+++ b/test/executionserver/test.py	(date 1745251458345)
@@ -0,0 +1,35 @@
+import unittest
+from abc import ABC, abstractmethod
+
+from model.schema import Table
+from test.duckdb.db import TestDuckDB, DuckDB
+from test.executionserver.testutils import TestExecutionServer
+
+
+class ExecutionServerTest(unittest.TestCase, ABC):
+
+    @classmethod
+    def setUpClass(cls):
+        cls.execution_server = TestExecutionServer("../executionserver")
+        cls.execution_server.start()
+        cls.duckdb = TestDuckDB()
+        cls.duckdb.start()
+
+    @classmethod
+    def tearDownClass(cls):
+        cls.execution_server.stop()
+        if cls.duckdb:
+            cls.duckdb.stop()
+            cls.duckdb = None
+
+    @classmethod
+    def create_table(cls, table: Table):
+        cls.duckdb.db.create_table(table)
+
+    @classmethod
+    def load_csv(cls, table: Table, path: str):
+        cls.duckdb.db.load_csv(table, path)
+
+    @classmethod
+    def get_duckdb_path(cls):
+        return cls.duckdb.db.get_db_path()
Index: legendql/ql.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/legendql/ql.py b/legendql/ql.py
new file mode 100644
--- /dev/null	(date 1745251458343)
+++ b/legendql/ql.py	(date 1745251458343)
@@ -0,0 +1,104 @@
+from __future__ import annotations
+
+from typing import Callable, Type, Dict
+
+from legendql import parser
+from model.metamodel import FromClause, OrderByClause, LimitClause, IntegerLiteral, OffsetClause, RenameClause, \
+    LeftJoinType, InnerJoinType, Runtime, DataFrame
+from legendql.parser import ParseType
+from model.metamodel import SelectionClause, ExtendClause, FilterClause, GroupByClause, JoinClause, JoinType
+from model.schema import Table, Database
+from legendql.query import Query
+
+
+class LegendQL:
+
+    def __init__(self, database: Database, table: Table):
+        self._query = Query.from_table(database, table)
+
+    @classmethod
+    def from_table(cls, database: Database, table: Table) -> LegendQL:
+        return LegendQL(database, table)
+
+    @classmethod
+    def from_db(cls, database: Database, table: str, columns: Dict[str, Type]) -> LegendQL:
+        return LegendQL.from_table(database, Table(table, columns))
+
+    @classmethod
+    def from_lh(cls, dataset: Table) -> LegendQL:
+        return LegendQL.from_table(Database("lakehouse", [dataset]), dataset)
+
+    def bind[R: Runtime](self, runtime: R) -> DataFrame:
+        return self._query.bind(runtime)
+
+    def eval[R: Runtime, T](self, runtime: R) -> DataFrame:
+        return self._query.eval(runtime)
+
+    def select(self, columns: Callable) -> LegendQL:
+        expression_and_table = parser.Parser.parse(columns, [self._query._table], ParseType.select)
+        self._query._add_clause(SelectionClause(expression_and_table[0]))
+        self._query._update_table(expression_and_table[1])
+        return self
+
+    def extend(self, columns: Callable) -> LegendQL:
+        expression_and_table = parser.Parser.parse(columns, [self._query._table], ParseType.extend)
+        self._query._add_clause(ExtendClause(expression_and_table[0]))
+        self._query._update_table(expression_and_table[1])
+        return self
+
+    def rename(self, columns: Callable) -> LegendQL:
+        expression_and_table = parser.Parser.parse(columns, [self._query._table], ParseType.rename)
+        self._query._add_clause(RenameClause(expression_and_table[0]))
+        self._query._update_table(expression_and_table[1])
+        return self
+
+    def filter(self, condition: Callable) -> LegendQL:
+        expression_and_table = parser.Parser.parse(condition, [self._query._table], ParseType.filter)
+        self._query._add_clause(FilterClause(expression_and_table[0]))
+        self._query._update_table(expression_and_table[1])
+        return self
+
+    def group_by(self, aggr: Callable) -> LegendQL:
+        expression_and_table = parser.Parser.parse(aggr, [self._query._table], ParseType.group_by)
+        self._query._add_clause(GroupByClause(expression_and_table[0]))
+        self._query._update_table(expression_and_table[1])
+        return self
+
+    def _join(self, lq: LegendQL, join: Callable, join_type: JoinType) -> LegendQL:
+        expression_and_table = parser.Parser.parse(join, [self._query._table, lq._query._table], ParseType.join)
+        self._query._add_clause(JoinClause(FromClause(lq._query._database.name, lq._query._table.table), join_type, expression_and_table[0]))
+        self._query._update_table(expression_and_table[1])
+        return self
+
+    def join(self, lq: LegendQL, join: Callable) -> LegendQL:
+        return self._join(lq, join, InnerJoinType())
+
+    def left_join(self, lq: LegendQL, join: Callable) -> LegendQL:
+        return self._join(lq, join, LeftJoinType())
+
+    def order_by(self, columns: Callable) -> LegendQL:
+        expression_and_table = parser.Parser.parse(columns, [self._query._table], ParseType.order_by)
+        self._query._add_clause(OrderByClause(expression_and_table[0]))
+        self._query._update_table(expression_and_table[1])
+        return self
+
+    def limit(self, limit: int) -> LegendQL:
+        clause = LimitClause(IntegerLiteral(limit))
+        self._query._add_clause(clause)
+        return self
+
+    def offset(self, offset: int) -> LegendQL:
+        clause = OffsetClause(IntegerLiteral(offset))
+        self._query._add_clause(clause)
+        return self
+
+    def take(self, offset: int, limit: int) -> LegendQL:
+        clause = OffsetClause(IntegerLiteral(offset))
+        self._query._add_clause(clause)
+
+        clause = LimitClause(IntegerLiteral(limit))
+        self._query._add_clause(clause)
+        return self
+
+    def get_table_definition(self) -> Table:
+        return self._query._table
\ No newline at end of file
Index: legendql/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/legendql/__init__.py b/legendql/__init__.py
new file mode 100644
--- /dev/null	(date 1745251458342)
+++ b/legendql/__init__.py	(date 1745251458342)
@@ -0,0 +1,15 @@
+from typing import Dict, Optional, Type
+
+from legendql.ql import LegendQL
+from model.schema import Database, Table
+
+
+def table(db_name: str, table_name: str, columns: Dict[str, Optional[Type]]) -> LegendQL:
+    db_table = Table(table_name, columns)
+    database = Database(db_name, [db_table])
+    return LegendQL.from_table(database, db_table)
+
+def db(db_name: str, tables: Dict[str, Dict[str, Optional[Type]]]) -> [LegendQL]:
+    db_tables = [Table(table_name, columns) for (table_name, columns) in tables.items()]
+    database = Database(db_name, db_tables)
+    return [LegendQL.from_table(database, db_table) for db_table in db_tables]
Index: legendql/parser.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/legendql/parser.py b/legendql/parser.py
new file mode 100644
--- /dev/null	(date 1745251458343)
+++ b/legendql/parser.py	(date 1745251458343)
@@ -0,0 +1,547 @@
+"""
+Lambda parser for the cloud-dataframe DSL.
+
+This module provides utilities for parsing Python lambda functions
+and converting them to SQL expressions.
+"""
+import ast
+import importlib
+import inspect
+from _ast import operator, arg
+from enum import Enum
+from typing import Callable, List, Union, Dict, Tuple
+
+from model.functions import StringConcatFunction
+from model.metamodel import Expression, BinaryExpression, BinaryOperator, \
+    ColumnReferenceExpression, BooleanLiteral, IfExpression, OrderByExpression, \
+    FunctionExpression, \
+    OperandExpression, AndBinaryOperator, OrBinaryOperator, IntegerLiteral, StringLiteral, EqualsBinaryOperator, \
+    NotEqualsBinaryOperator, LessThanBinaryOperator, LessThanEqualsBinaryOperator, GreaterThanBinaryOperator, \
+    GreaterThanEqualsBinaryOperator, InBinaryOperator, NotInBinaryOperator, IsBinaryOperator, IsNotBinaryOperator, \
+    AddBinaryOperator, SubtractBinaryOperator, MultiplyBinaryOperator, DivideBinaryOperator, BitwiseOrBinaryOperator, \
+    BitwiseAndBinaryOperator, DateLiteral, GroupByExpression, \
+    DescendingOrderType, ComputedColumnAliasExpression, AscendingOrderType, ColumnAliasExpression, LambdaExpression, \
+    VariableAliasExpression, MapReduceExpression, UnaryExpression, NotUnaryOperator, ModuloFunction, ExponentFunction
+from model.schema import Table
+
+class ParseType(Enum):
+    extend = "extend"
+    join = "join"
+    rename = "rename"
+    select = "select"
+    group_by = "group_by"
+    filter = "filter"
+    order_by = "order_by"
+    lambda_body = "lambda_body"
+    over = "over"
+
+class Parser:
+
+    @staticmethod
+    def parse[E: Expression](func: Callable, tables: [Table], ptype: ParseType) -> Tuple[Union[E, List[E], Table]]:
+        """
+        Parse a lambda function and convert it to an Expression or list of Expressions.
+
+        Args:
+            func: The lambda function to parse. Can be:
+                - A lambda that returns a boolean expression (e.g., lambda x: x.age > 30)
+                - A lambda that returns a column reference (e.g., lambda x: x.name)
+                - A lambda that returns an array of column references (e.g., lambda x: [x.name, x.age])
+                - A lambda that returns tuples with sort direction (e.g., lambda x: [(x.department, Sort.DESC)])
+            tables: the current LegendQL query context
+            ptype: The LegendQL function calling this parser (extend, join, select ..)
+
+        Returns:
+            An Expression or list of Expressions representing the lambda function
+        """
+        # Get the source code of the lambda function
+        lambda_node = Parser._get_lambda_node(func)
+        lambda_args = lambda_node.args.args
+
+        match ptype:
+            case ParseType.select:
+                Parser._validate_lambda_args_length(lambda_args, 1)
+                new_table = Table(tables[0].table, {})
+                return (Parser._parse_select(lambda_node.body, new_table), new_table)
+            case ParseType.filter:
+                Parser._validate_lambda_args_length(lambda_args, 1)
+                new_table = Table(tables[0].table, tables[0].columns.copy())
+                return (Parser._parse_filter(lambda_node.body, lambda_args, new_table), new_table)
+            case ParseType.extend:
+                Parser._validate_lambda_args_length(lambda_args, 1)
+                new_table = Table(tables[0].table, tables[0].columns.copy())
+                return (Parser._parse_extend(lambda_node.body, lambda_args, new_table), new_table)
+            case ParseType.join:
+                new_table = "_".join(map(lambda s: s.table, tables))
+                new_table = Table(new_table, {})
+                for s in tables:
+                    new_table.columns.update(s.columns.copy())
+                Parser._validate_lambda_args_length(lambda_args, 2)
+                return (Parser._parse_join(lambda_node.body, lambda_args, new_table), new_table)
+            case ParseType.rename:
+                new_table = Table(tables[0].table, tables[0].columns.copy())
+                Parser._validate_lambda_args_length(lambda_args, 1)
+                return (Parser._parse_rename(lambda_node.body), new_table)
+            case ParseType.group_by:
+                Parser._validate_lambda_args_length(lambda_args, 1)
+                new_table = Table(tables[0].table, tables[0].columns.copy())
+                return (Parser._parse_group_by(lambda_node.body, lambda_args, new_table), new_table)
+            case ParseType.order_by:
+                Parser._validate_lambda_args_length(lambda_args, 1)
+                new_table = Table(tables[0].table, tables[0].columns.copy())
+                return (Parser._parse_order_by(lambda_node.body, lambda_args), new_table)
+            case ParseType.over:
+                Parser._validate_lambda_args_length(lambda_args, 1)
+                new_table = Table(tables[0].table, tables[0].columns.copy())
+                return (Parser._parse_over(lambda_node.body, lambda_args), new_table)
+            case _:
+                raise ValueError(f"Unknown ParseType: {ptype}")
+
+    @staticmethod
+    def _get_lambda_node(func):
+        source_lines, _ = inspect.getsourcelines(func)
+        source_text = ''.join(source_lines).strip().replace("\n", "")
+
+        try:
+            # if it is lambda on own line this should work
+            source_ast = ast.parse(source_text)
+        except:
+            # fluent api way
+            idx = source_text.find("lambda")
+            source_text = source_text[idx:len(source_text) - 1]
+
+            try:
+                # fluent api way
+                source_ast = ast.parse(source_text)
+            except:
+                # is it on the last line? try to strip out one more
+                source_text = source_text[:len(source_text) - 1]
+
+                try:
+                    source_ast = ast.parse(source_text)
+                except:
+                    raise ValueError(f"Could not get Lambda func: {source_text}")
+
+        return next((node for node in ast.walk(source_ast) if isinstance(node, ast.Lambda)), None)
+
+    @staticmethod
+    def _validate_lambda_args_length(args: [arg], length: int) -> None:
+        if len(args) != length:
+            raise ValueError(f"Lambda MUST have exactly {length} argument(s): {args}")
+
+    @staticmethod
+    def _parse_select(node: ast.AST, new_table: Table) -> [ColumnReferenceExpression]:
+        if isinstance(node, ast.List):
+            return [item for sublist in map(lambda n: Parser._parse_select(n, new_table), node.elts) for item in sublist]
+
+        if isinstance(node, ast.Attribute) and isinstance(node.value, ast.Name):
+            # need to be able to infer type
+            new_table.columns[node.attr] = None
+            return [ColumnReferenceExpression(name=node.attr)]
+
+        raise ValueError(f"Unsupported Column Reference {node.value}")
+
+    @staticmethod
+    def _parse_filter(node: ast.AST, args: [arg], new_table: Table) -> LambdaExpression:
+        return LambdaExpression(list(map(lambda a: a.arg, args)), Parser._parse_lambda_body(node, ParseType.lambda_body, new_table))
+
+    @staticmethod
+    def _parse_extend(node: ast.AST, args: [arg], new_table: Table) -> List[ComputedColumnAliasExpression]:
+        if isinstance(node, ast.List):
+            new_implicit_aliases = {}
+            extends = []
+            for elt in node.elts:
+                node_and_new_aliases = Parser._parse_single_extend(elt, args, new_implicit_aliases, new_table)
+                new_implicit_aliases.update(node_and_new_aliases[1])
+                extends.append(node_and_new_aliases[0])
+            return extends
+
+        return [Parser._parse_single_extend(node, args, {}, new_table)[0]]
+
+    @staticmethod
+    def _parse_single_extend(node: ast.AST, args: [arg], implicit_aliases: Dict[str, str], new_table: Table) -> Tuple[ComputedColumnAliasExpression, Dict[str, str]]:
+        if isinstance(node, ast.NamedExpr) and isinstance(node.target, ast.Name):
+            new_table.columns[node.target.id] = None
+            return (ComputedColumnAliasExpression(node.target.id, LambdaExpression(list(map(lambda a: a.arg, args)), Parser._parse_lambda_body(node.value, args, new_table, implicit_aliases))), {node.target.id: args[0].arg})
+
+        raise ValueError(f"Not a valid extend statement {node.value}")
+
+    @staticmethod
+    def _parse_join(node: ast.AST, args: [arg], new_table: Table) -> List[ColumnReferenceExpression]:
+        return LambdaExpression(list(map(lambda a: a.arg, args)), Parser._parse_lambda_body(node, ParseType.lambda_body, new_table))
+
+    @staticmethod
+    def _parse_rename(node: ast.AST) -> List[ColumnReferenceExpression]:
+        if isinstance(node, ast.List) or isinstance(node, ast.Tuple):
+            return [item for sublist in map(lambda n: Parser._parse_rename(n), node.elts) for item in sublist]
+
+        if isinstance(node, ast.NamedExpr) and isinstance(node.target, ast.Name) and isinstance(node.value, ast.Attribute):
+            return [ColumnAliasExpression(node.target.id, ColumnReferenceExpression(node.value.attr))]
+
+        raise ValueError(f"Unsupported Rename {node}")
+
+    @staticmethod
+    def _parse_group_by(node: ast.AST, args: [arg], new_table: Table) -> Tuple[GroupByExpression, Table]:
+        if isinstance(node, ast.Call) and isinstance(node.func, ast.Name) and node.func.id == "aggregate":
+            if len(node.args) != 2 and len(node.args) != 3:
+                raise ValueError(f"An aggregate function requires 2 or 3 arguments: {node.args}")
+
+            # capture the columns we're going to group by
+            group_by_table = Table(new_table.table, {})
+            selections = Parser._parse_select(node.args[0], group_by_table)
+
+            expressions_and_aliases = list(map(lambda n: Parser._parse_group_by_map_aggregate(n, args, new_table, group_by_table), node.args[1].elts if isinstance(node.args[1], ast.List) or isinstance(node.args[1], ast.Tuple) else [node.args[1]]))
+            expressions = list(map(lambda e: e[0], expressions_and_aliases))
+            implicit_aliases = list(map(lambda e: e[1], expressions_and_aliases))
+
+            having = Parser._parse_lambda_body(node.args[2], args, group_by_table, implicit_aliases) if len(node.args) == 3 else None
+            new_table.columns = group_by_table.columns
+            return GroupByExpression(selections, expressions, having)
+
+        raise ValueError(f"Unsupported GroupBy expression {node}")
+
+    @staticmethod
+    def _parse_group_by_map_aggregate(node: ast.AST, args: [arg], full_table: Table, target_table: Table) -> Tuple[Expression, dict[str, str]]:
+        if isinstance(node, ast.NamedExpr):
+            computed_column = node.target.id
+            target_table.columns[computed_column] = None
+            full_table.columns[computed_column] = None
+            # since pure groupBy's map/reduce is really map/apply_function, we need to assume the first node is a Call
+            # note that there is only one arg for this lambda
+            if isinstance(node.value, ast.Call):
+                map_expression = LambdaExpression(list(map(lambda a: a.arg, args)), Parser._parse_lambda_body(node.value.args[0], args, full_table))
+                # very brittle, lots more checks needed here
+                module = importlib.import_module("model.functions")
+                class_ = getattr(module, f"{node.value.func.id.title()}Function")
+                function_instance = class_()
+                function_argument = args[0].arg
+                reduce_expression = LambdaExpression(list(map(lambda a: a.arg, args)), FunctionExpression(function_instance, [VariableAliasExpression(function_argument)]))
+                return (ComputedColumnAliasExpression(computed_column, MapReduceExpression(map_expression, reduce_expression)), {computed_column: function_argument})
+
+        raise ValueError(f"Unsupported GroupBy expression {node}")
+
+    @staticmethod
+    def _parse_order_by(node: ast.AST, args: [arg]) -> [OrderByExpression]:
+        if isinstance(node, ast.List):
+            return [item for sublist in map(lambda n: Parser._parse_order_by(n, args), node.elts) for item in sublist]
+
+        order = AscendingOrderType()
+        clause = node
+        if isinstance(node, ast.UnaryOp):
+            clause = node.operand
+            if isinstance(node.op, ast.UAdd):
+                order = AscendingOrderType()
+            if isinstance(node.op, ast.USub):
+                order = DescendingOrderType()
+
+        if isinstance(clause, ast.Attribute):
+            return [OrderByExpression(order, ColumnReferenceExpression(clause.attr))]
+
+        raise ValueError(f"Not a valid sort statement {clause}")
+
+    @staticmethod
+    def _parse_over(node: ast.AST, tables: Dict[str, Table], args: [arg]) -> Expression:
+        print(ast.dump(node))
+        # window = lambda r: (avg_val :=
+        #                     over(r.location, avg(r.salary), sort=[r.emp_name, -r.location], frame=rows(0, unbounded())))
+
+        # columns: Union[ast.expr, list[ast.expr]],
+        # functions: Union[AggregationFunction, WindowFunction, list[AggregationFunction], list[WindowFunction]],
+        # sort: Optional[Union[ast.expr, list[ast.expr]]] = None,
+        # frame: Optional[Frame] = None,
+        # qualify: Optional[bool] = None):
+
+        if isinstance(node, ast.NamedExpr):
+            alias = node.target.id
+            if isinstance(node.value, ast.Call) and node.value.func.id == "over":
+                print(ast.dump(node.value.args))
+
+        raise NotImplementedError()
+
+    @staticmethod
+    def _parse_lambda_body(node: ast.AST, args: [arg], new_table: Table, implicit_aliases: dict[str, str] = None) -> Expression:
+        if node is None:
+            raise ValueError("node in Parser._parse_expression is None")
+
+        if isinstance(node, ast.NamedExpr):
+            new_table.columns[node.target.id] = None
+            return ColumnAliasExpression(node.target.id, Parser._parse_lambda_body(node.value, args, new_table, implicit_aliases))
+
+        if isinstance(node, ast.Compare):
+            # Handle comparison operations (e.g., x > 5, y == 'value')
+            left = Parser._parse_lambda_body(node.left, args, new_table, implicit_aliases)
+
+            # We only handle the first comparator for simplicity
+            # In a real implementation, we would handle multiple comparators
+            op = node.ops[0]
+            right = Parser._parse_lambda_body(node.comparators[0], args, new_table, implicit_aliases)
+
+            comp_op = Parser._get_comparison_operator(op)
+
+            # Ensure left and right are Expression objects, not lists or tuples
+            if isinstance(left, list) or isinstance(left, tuple):
+                raise ValueError(f"Unsupported Compare object {left}")
+            if isinstance(right, list) or isinstance(right, tuple):
+                raise ValueError(f"Unsupported Compare object {right}")
+
+            return BinaryExpression(left=OperandExpression(left), operator=comp_op, right=OperandExpression(right))
+
+        elif isinstance(node, ast.BinOp):
+            # Handle binary operations (e.g., x + y, x - y, x * y)
+            left = Parser._parse_lambda_body(node.left, args, new_table, implicit_aliases)
+            right = Parser._parse_lambda_body(node.right, args, new_table, implicit_aliases)
+
+            if isinstance(node.op, ast.Mod):
+                return FunctionExpression(parameters=[left, right], function=ModuloFunction())
+            elif isinstance(node.op, ast.Pow):
+                return FunctionExpression(parameters=[left, right], function=ExponentFunction())
+
+            comp_op = Parser._get_binary_operator(node.op)
+
+            # Ensure left and right are Expression objects, not lists or tuples
+            if isinstance(left, list) or isinstance(left, tuple):
+                raise ValueError(f"Unsupported BinOp object {left}")
+            if isinstance(right, list) or isinstance(right, tuple):
+                raise ValueError(f"Unsupported BinOp object {right}")
+
+            return BinaryExpression(left=OperandExpression(left), operator=comp_op, right=OperandExpression(right))
+
+        elif isinstance(node, ast.BoolOp):
+            # Handle boolean operations (e.g., x and y, x or y)
+            values = [Parser._parse_lambda_body(val, args, new_table, implicit_aliases) for val in node.values]
+
+            # Combine the values with the appropriate operator
+            comp_op = AndBinaryOperator() if isinstance(node.op, ast.And) else OrBinaryOperator()
+
+            # Ensure all values are Expression objects, not lists or tuples
+            processed_values = []
+            for val in values:
+                if isinstance(val, list) or isinstance(val, tuple):
+                    raise ValueError(f"Unsupported BoolOp object {val}")
+                else:
+                    processed_values.append(val)
+
+            # Start with the first two values
+            result = BinaryExpression(left=OperandExpression(processed_values[0]), operator=comp_op, right=OperandExpression(processed_values[1]))
+
+            # Add the remaining values
+            for value in processed_values[2:]:
+                result = BinaryExpression(left=OperandExpression(result), operator=comp_op, right=OperandExpression(value))
+
+            return result
+
+        elif isinstance(node, ast.Attribute):
+            # Handle column references (e.g. x.column_name)
+            if isinstance(node.value, ast.Name):
+                # validate the column name
+                if not new_table.validate_column(node.attr):
+                    raise ValueError(f"Column '{node.attr}' not found in table '{new_table}'")
+
+                return ColumnAliasExpression(alias=node.value.id, reference=ColumnReferenceExpression(name=node.attr))
+
+        elif isinstance(node, ast.Name):
+            if node.id == "True":
+                from model.metamodel import LiteralExpression
+                return LiteralExpression(BooleanLiteral(True))
+            elif node.id == "False":
+                from model.metamodel import LiteralExpression
+                return LiteralExpression(BooleanLiteral(False))
+            else:
+                alias = implicit_aliases.get(node.id, None) if implicit_aliases else None
+                if alias:
+                    return ColumnAliasExpression(alias, ColumnReferenceExpression(node.id))
+                return ColumnReferenceExpression(node.id)
+
+        elif isinstance(node, ast.Constant):
+            # Handle literal values (e.g., 5, 'value', True)
+            from model.metamodel import LiteralExpression
+            if isinstance(node.value, int):
+                return LiteralExpression(IntegerLiteral(node.value))
+            if isinstance(node.value, bool):
+                return LiteralExpression(BooleanLiteral(node.value))
+            if isinstance(node.value, str):
+                return LiteralExpression(StringLiteral(node.value))
+
+            raise ValueError(f"Cannot convert literal type {type(node.value)}")
+
+        elif isinstance(node, ast.UnaryOp):
+            # Handle unary operations (e.g., not x)
+            operand = Parser._parse_lambda_body(node.operand, args, new_table, implicit_aliases)
+
+            # Ensure operand is an Expression object, not a list or tuple
+            if isinstance(operand, list) or isinstance(operand, tuple):
+                # Use a fallback for list/tuple values in unary operations
+                raise ValueError(f"Unsupported expression to UnaryOp: {operand}")
+
+            if isinstance(node.op, ast.Not):
+                return UnaryExpression(operator=NotUnaryOperator(), expression=OperandExpression(operand))
+            else:
+                # Other unary operations (e.g., +, -)
+                # In a real implementation, we would handle this more robustly
+                return Parser._parse_lambda_body(node.operand, args, new_table, implicit_aliases)
+
+        elif isinstance(node, ast.IfExp):
+            # Handle conditional expressions (e.g., x if y else z)
+            # In a real implementation, we would handle this more robustly
+            test = Parser._parse_lambda_body(node.test, args, new_table, implicit_aliases)
+            body = Parser._parse_lambda_body(node.body, args, new_table, implicit_aliases)
+            orelse = Parser._parse_lambda_body(node.orelse, args, new_table, implicit_aliases)
+
+            # Ensure all values are Expression objects, not lists or tuples
+            if isinstance(test, list) or isinstance(test, tuple):
+                raise ValueError(f"Unsupported IfExp: {test}")
+            if isinstance(body, list) or isinstance(body, tuple):
+                raise ValueError(f"Unsupported IfExp: {body}")
+            if isinstance(orelse, list) or isinstance(orelse, tuple):
+                raise ValueError(f"Unsupported IfExp: {orelse}")
+
+            # Create a CASE WHEN expression
+            return IfExpression(test=test, body=body, orelse=orelse)
+
+        elif isinstance(node, ast.List):
+            # Handle tuples and lists (e.g., (1, 2, 3), [1, 2, 3])
+            # This is used for array returns in lambdas like lambda x: [x.name, x.age]
+            elements = []
+            for elt in node.elts:
+                elements.append(Parser._parse_lambda_body(elt, args, new_table, implicit_aliases))
+            return elements
+
+        elif isinstance(node, ast.Tuple):
+            # Handle Join with rename ( x.col1 == y.col1, [ (x_col1 := x.col1 ), (y_col1 := y.col1 ) ]
+            elements = []
+            for elt in node.elts:
+                elements.append(Parser._parse_lambda_body(elt, args, new_table, implicit_aliases))
+            return elements
+
+        elif isinstance(node, ast.Call):
+            # Handle function calls (e.g., sum(x.col1 - x.col2))
+            args_list = []
+            # kwargs = {}
+
+            if isinstance(node.func, ast.Name):
+                # Parse the arguments to the function
+
+                for arg in node.args:
+                    parsed_arg = Parser._parse_lambda_body(arg, args, new_table, implicit_aliases)
+                    args_list.append(parsed_arg)
+
+                # Handle keyword arguments
+                # kwargs = {}
+                for kw in node.keywords:
+                    parsed_kw = Parser._parse_lambda_body(kw.value, args, new_table, implicit_aliases)
+                    # kwargs[kw.arg] = parsed_kw
+                    args_list.append(parsed_kw)
+
+            else:
+                ValueError(f"Unsupported function type: {node.func}")
+
+            if node.func.id == "date":
+                from model.metamodel import LiteralExpression
+                from datetime import date
+                # random use of date to prevent auto-format refactorings deleting the import
+                date(1999, 1, 1)
+                compiled = compile(ast.fix_missing_locations(ast.Expression(body=node)), '', 'eval')
+                val = eval(compiled, None, None)
+                return LiteralExpression(literal=DateLiteral(val))
+
+            #if node.func.id not in known_functions:
+            #    ValueError(f"Unknown function name: {node.func.id}")
+
+            # very brittle, lots more checks needed here
+            module = importlib.import_module("model.functions")
+            class_ = getattr(module, f"{node.func.id.title()}Function")
+            instance = class_()
+            return FunctionExpression(instance, parameters=args_list)
+
+        elif isinstance(node, ast.JoinedStr):
+            # Handle fstring (e.g. f"hello{blah}")
+            # In a real implementation, we would handle this more robustly
+            expr = []
+            for value in node.values:
+                if isinstance(value, ast.Constant):
+                    expr.append(Parser._parse_lambda_body(value, args, new_table, implicit_aliases))
+                elif isinstance(value, ast.FormattedValue):
+                    if value.format_spec is not None:
+                        raise ValueError(f"Format Spec Not Supported: {value.format_spec}")
+                    else:
+                        expr.append(Parser._parse_lambda_body(value.value, args, new_table, implicit_aliases))
+
+            return FunctionExpression(StringConcatFunction(), expr)
+
+        elif isinstance(node, ast.Subscript):
+            # Handle subscript operations (e.g., x[0], x['key'])
+            # In a real implementation, we would handle this more robustly
+            raise ValueError(f"Unsupported expression: {node}")
+
+        elif isinstance(node, ast.Dict):
+            # Handle dictionaries (e.g., {'a': 1, 'b': 2})
+            # In a real implementation, we would handle this more robustly
+            raise ValueError(f"Unsupported expression: {node}")
+
+        elif isinstance(node, ast.Set):
+            # Handle sets (e.g., {1, 2, 3})
+            # In a real implementation, we would handle this more robustly
+            raise ValueError(f"Unsupported expression: {node}")
+
+        elif isinstance(node, ast.ListComp) or isinstance(node, ast.SetComp) or isinstance(node, ast.DictComp) or isinstance(node, ast.GeneratorExp):
+            # Handle comprehensions (e.g., [x for x in y], {x: y for x in z})
+            # In a real implementation, we would handle this more robustly
+            raise ValueError(f"Unsupported expression: {node}")
+
+        else:
+            # Throw
+            raise ValueError(f"Unsupported expression: {ast.dump(node)}")
+
+        return ColumnReferenceExpression("PLACEHOLDER")
+
+    @staticmethod
+    def _get_comparison_operator(op: ast.cmpop) -> BinaryOperator:
+        """
+        Convert an AST comparison operator to a SQL operator.
+
+        Args:
+            op: The AST comparison operator
+
+        Returns:
+            The equivalent SQL operator
+        """
+        if isinstance(op, ast.Eq):
+            return EqualsBinaryOperator()
+        elif isinstance(op, ast.NotEq):
+            return NotEqualsBinaryOperator()
+        elif isinstance(op, ast.Lt):
+            return LessThanBinaryOperator()
+        elif isinstance(op, ast.LtE):
+            return LessThanEqualsBinaryOperator()
+        elif isinstance(op, ast.Gt):
+            return GreaterThanBinaryOperator()
+        elif isinstance(op, ast.GtE):
+            return GreaterThanEqualsBinaryOperator()
+        elif isinstance(op, ast.In):
+            return InBinaryOperator()
+        elif isinstance(op, ast.NotIn):
+            return NotInBinaryOperator()
+        elif isinstance(op, ast.Is):
+            return IsBinaryOperator()
+        elif isinstance(op, ast.IsNot):
+            return IsNotBinaryOperator()
+        else:
+            raise ValueError(f"Unsupported comparison operator {op}")
+
+    @staticmethod
+    def _get_binary_operator(op: operator) -> BinaryOperator:
+        # Map Python operators to Binary operators
+        if isinstance(op, ast.Add):
+            return AddBinaryOperator()
+        elif isinstance(op, ast.Sub):
+            return SubtractBinaryOperator()
+        elif isinstance(op, ast.Mult):
+            return MultiplyBinaryOperator()
+        elif isinstance(op, ast.Div):
+            return DivideBinaryOperator()
+        elif isinstance(op, ast.BitOr):
+            return BitwiseOrBinaryOperator()
+        elif isinstance(op, ast.BitAnd):
+            return BitwiseAndBinaryOperator()
+        else:
+            raise ValueError(f"Unsupported binary operator {op}")
Index: legendql/examples.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/legendql/examples.py b/legendql/examples.py
new file mode 100644
--- /dev/null	(date 1745251458342)
+++ b/legendql/examples.py	(date 1745251458342)
@@ -0,0 +1,199 @@
+import legendql as lq
+from legendql.functions import over, avg, rows, aggregate, unbounded, count, left
+
+
+'''
+    Example DSL with Join using Fluent-style API
+'''
+[emp, dep] = lq.db("db", {
+    "employees": {"id": int, "name": str, "dept_id": str, "salary": float, "title": str},
+    "department": {"id": int, "name": str, "city": str, "code": str, "location": str}
+})
+
+(emp
+ .filter(lambda r: r.id > 10)
+ .left_join(dep, lambda e, d: (
+    e.dept_id == d.id,
+    (department_name := d.name, department_id := d.id)))
+ .extend(lambda r:[
+    (ids := r.id + r.dept_id),
+    (avg_val := over(r.location, avg(r.salary),
+    sort=[r.name, -r.location],
+    frame=rows(0, unbounded())))])
+ .group_by(lambda r: aggregate(
+    [ r.id, r.name ],
+    [sum_salary := sum(r.salary), count_dept := count(r.department_name) ],
+    having=sum_salary > 100_000))
+ .filter(lambda r: r.id > 100)
+ .extend(lambda r: (calc_col := r.id + r.sum_salary)))
+
+for clause in emp._query._clauses:
+    print(clause)
+
+
+'''
+    PRQL example from https://prql-lang.org/book/index.html
+    Using reassignment for each expression
+'''
+q = lq.table('db', "employees",
+             columns={"id": int, "name": str, "title": str, "country": str, "dept_id": str, "salary": float, "start_date": str, "benefits": str})
+
+q = q.filter(lambda e: e.start_date > '2021-01-01')
+q = q.extend(lambda e: [
+        (gross_salary := e.salary + 10),
+        (gross_cost := gross_salary + e.benefits) ])
+q = q.filter(lambda e: e.gross_cost > 0)
+q = q.group_by(lambda e: aggregate(
+        [e.title, e.country],
+        [avg_gross_salary := avg(e.gross_salary), sum_gross_cost := sum(e.gross_cost)],
+        having=sum_gross_cost > 100_000))
+q = q.extend(lambda e: (new_id := f"{e.title}_{e.country}"))
+q = q.extend(lambda e: (country_code := left(e.country, 2)))
+q = q.order_by(lambda e: [e.sum_gross_cost, -e.country])
+q = q.limit(10)
+
+for clause in q._query._clauses:
+    print(clause)
+
+'''
+    Same PRQL example using Fluent API, spacing and line breaks are important
+'''
+
+emp = lq.table("db", "employees",
+               {"id": int, "name": str, "title": str, "country": str, "dept_id": str, "salary": float, "start_date": str, "benefits": str})
+
+(emp
+ .filter(lambda e: e.start_date > '2021-01-01')
+ .extend(lambda e: [
+    (gross_salary := e.salary + 10),
+    (gross_cost := gross_salary + e.benefits) ])
+ .filter(lambda e: e.gross_cost > 0)
+ .group_by(lambda e: aggregate(
+    [e.title, e.country],
+    [avg_gross_salary := avg(e.gross_salary), sum_gross_cost := sum(e.gross_cost)],
+    having=sum_gross_cost > 100_000))
+ .extend(lambda e: (new_id := f"{e.title}_{e.country}"))
+ .extend(lambda e: (country_code := left(e.country, 2)))
+ .order_by(lambda e: [e.sum_gross_cost, -e.country])
+ .limit(10))
+
+for clause in emp._query._clauses:
+    print(clause)
+
+
+'''
+    Example Window as an expression
+'''
+emp = lq.table("db", "employees",
+               {"id": int, "name": str, "dept_id": str, "salary": float, "location": str})
+
+(emp
+ .extend(lambda r: (
+    avg_val := over(
+        r.location,
+        (avg_val := avg(r.salary)),
+        sort=[r.name, -r.location],
+        frame=rows(0, unbounded()),
+        qualify=avg_val > 100_000)))
+ )
+
+for clause in emp._query._clauses:
+    print(clause)
+
+emp = lq.table("db", "employees",
+               {"id": int, "name": str, "dept_id": str, "salary": float, "location": str})
+
+emp = emp.extend(lambda r: (avg_val := over(r.location, avg(r.salary))))
+
+for clause in emp._query._clauses:
+    print(clause)
+
+emp = lq.table("db", "employees", {"id": int, "name": str, "title": str, "dept_id": str, "salary": float})
+emp = emp.group_by(lambda r: aggregate(r.title, avg_salary := avg(r.salary)))
+
+for clause in emp._query._clauses:
+    print(clause)
+
+emp = lq.table("db", "employees", {"id": int, "name": str, "title": str, "dept_id": str, "salary": float})
+emp = emp.group_by(lambda r: aggregate([r.title, r.dept_id], avg_salary := avg(r.salary)))
+
+for clause in emp._query._clauses:
+    print(clause)
+
+emp = lq.table("db", "employees", {"id": int, "name": str, "title": str, "dept_id": str, "salary": float})
+emp = emp.group_by(lambda r: aggregate(r.title, avg_salary := avg(r.salary), having=avg_salary > 100_000))
+
+for clause in emp._query._clauses:
+    print(clause)
+
+# emp = lq.from_("db", "employees", {"id": int, "name": str, "dept_id": str, "salary": float, "title": str})
+# dep = lq.from_("db", "department", {"id": int, "name": str, "city": str, "code": str, "location": str})
+# loc = lq.from_("db", "location", {"id": int, "name": str, "country": str, "code": str})
+
+[emp, dep, loc] = lq.db("db", {
+    "employees": {"id": int, "name": str, "dept_id": str, "salary": float, "title": str},
+    "department": {"id": int, "name": str, "city": str, "code": str, "location": str},
+    "location": {"id": int, "name": str, "country": str, "code": str}
+})
+
+emp = (emp
+ .left_join(dep, lambda e, d: (e.dept_id == d.id, (new_dept_id := d.id, new_dept_name := d.name)))
+ .left_join(loc, lambda d, l: (d.city == l.id, (new_loc_id := l.id, new_loc_name := l.name, new_loc_code := l.code))))
+
+for clause in emp._query._clauses:
+    print(clause)
+
+[emp, dep, loc] = lq.db("db", {
+    "employees": {"id": int, "name": str, "dept_id": str, "salary": float},
+    "department": {"id": int, "name": str, "city": str, "code": str},
+    "location": {"id": int, "name": str, "country": str, "code": str}
+})
+
+emp = (emp
+ .left_join(dep, lambda e, d: (e.dept_id == d.id, [(new_dept_id := d.id), (new_dept_name := d.name)]))
+ .left_join(loc, lambda d, l: (d.city == l.id, (new_loc_id := l.id, new_loc_name := l.name, new_loc_code := l.code))))
+
+for clause in emp._query._clauses:
+    print(clause)
+
+dep = lq.table("db", "department", {"id": int, "name": str, "city": str, "code": str})
+dep.extend(lambda e: (id_plus_one := e.id + 1))
+
+print(dep._query._table.columns)
+
+for clause in dep._query._clauses:
+    print(clause)
+
+[emp, dep] = lq.db("db",{
+    "employees": {"id": int, "name": str, "dept_id": str, "salary": float},
+    "department": {"id": int, "name": str, "city": str, "code": str}
+})
+emp = emp.left_join(dep, lambda e, d: (e.dept_id == d.id, [(new_dept_id := d.id), (new_dept_name := d.name)]))
+
+print(emp._query._table.columns)
+
+dep = lq.table("db","department", {"id": int, "name": str, "city": str, "code": str})
+dep.rename(lambda d: (new_dept_id := d.id, new_dept_name := d.name))
+
+print(dep._query._table.columns)
+
+for clause in dep._query._clauses:
+    print(clause)
+
+
+dep = lq.table("db","department", {"id": int, "name": str, "city": str, "code": str})
+dep.select(lambda d: [d.id, d.name])
+
+print(dep._query._table.columns)
+
+for clause in dep._query._clauses:
+    print(clause)
+
+
+dep = lq.table("db", "department", {"id": int, "name": str, "city": str, "code": str})
+dep.group_by(lambda d: aggregate([d.id, d.name], [sum_test := sum(d.code), count_test := count(d.city)]))
+
+print(dep._query._table.columns)
+
+for clause in dep._query._clauses:
+    print(clause)
Index: runtime/pure/executionserver/runtime.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/runtime/pure/executionserver/runtime.py b/runtime/pure/executionserver/runtime.py
new file mode 100644
--- /dev/null	(date 1745251458344)
+++ b/runtime/pure/executionserver/runtime.py	(date 1745251458344)
@@ -0,0 +1,55 @@
+import json
+from dataclasses import dataclass
+from typing import List, Any
+
+import requests
+
+from dialect.purerelation.dialect import PureRuntime
+from model.metamodel import Clause
+from model.schema import Table, Database
+from runtime.pure.db.type import DatabaseType
+
+@dataclass
+class TDS:
+    relation: str
+    sql: str
+    header: List[str]
+    rows: List[List[Any]]
+    pass
+
+@dataclass
+class ExecutionServerRuntime(PureRuntime):
+    database_type: DatabaseType
+    host: str
+    database: Database
+
+    def eval(self, clauses: List[Clause]) -> TDS:
+        lam = self.executable_to_string(clauses)
+        model = self._generate_model()
+        query = self._parse_lambda(lam)
+        pmcd = self._parse_model(model)
+        runtime = pmcd["elements"][0]["runtimeValue"]
+
+        execution_input = {"clientVersion": "vX_X_X", "context": {"_type": "BaseExecutionContext"}, "function": query, "runtime": runtime, "model": pmcd}
+        return self._parse_execution_response(lam, self._execute(execution_input))
+
+    def _parse_model(self, model: str) -> dict:
+        return requests.post(self.host + "/api/pure/v1/grammar/grammarToJson/model", data=model).json()
+
+    def _parse_lambda(self, lam: str) -> dict:
+        return requests.post(self.host + "/api/pure/v1/grammar/grammarToJson/lambda", data="|" + lam).json()
+
+    def _execute(self, input: dict) -> dict:
+        return requests.post(self.host + "/api/pure/v1/execution/execute?serializationFormat=DEFAULT", json=input).json()
+
+    def _generate_model(self) -> str:
+        return self.database_type.generate_model(self.name, self.database)
+
+    @staticmethod
+    def _parse_execution_response(relation: str, result: dict) -> TDS:
+        sql = result["activities"][0]["sql"]
+        headers = result["result"]["columns"]
+        rows = []
+        for row in result["result"]["rows"]:
+            rows.append(row["values"])
+        return TDS(relation, sql, headers, rows)
\ No newline at end of file
Index: runtime/pure/repl/runtime.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/runtime/pure/repl/runtime.py b/runtime/pure/repl/runtime.py
new file mode 100644
--- /dev/null	(date 1745251458344)
+++ b/runtime/pure/repl/runtime.py	(date 1745251458344)
@@ -0,0 +1,10 @@
+from typing import List
+
+from dialect.purerelation.dialect import PureRuntime
+from model.metamodel import Clause
+from runtime.pure.repl.repl_utils import send_to_repl
+
+
+class ReplRuntime(PureRuntime):
+    def eval(self, clauses: List[Clause]) -> str:
+        return send_to_repl(self.executable_to_string(clauses))
Index: runtime/pure/repl/repl_utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/runtime/pure/repl/repl_utils.py b/runtime/pure/repl/repl_utils.py
new file mode 100644
--- /dev/null	(date 1745251458344)
+++ b/runtime/pure/repl/repl_utils.py	(date 1745251458344)
@@ -0,0 +1,198 @@
+"""
+Utilities for interacting with the Pure Relation REPL.
+"""
+import os
+import csv
+import tempfile
+import sys
+import subprocess
+import time
+import json
+import re
+import threading
+import queue
+
+repl_process = None
+repl_output_queue = queue.Queue()
+repl_ready = threading.Event()
+
+def start_repl():
+    """Start the REPL and keep it running."""
+    global repl_process
+
+    if repl_process is not None:
+        print("REPL is already running.")
+        return
+
+    cmd = "java -jar ../../legend-engine-repl-relational-4.74.1-SNAPSHOT.jar"
+    print(f"Starting REPL with command: {cmd}")
+
+    repl_process = subprocess.Popen(
+        cmd,
+        shell=True,
+        stdin=subprocess.PIPE,
+        stdout=subprocess.PIPE,
+        stderr=subprocess.PIPE,
+        text=True,
+        bufsize=1
+    )
+
+    def read_output():
+        while repl_process.poll() is None:
+            line = repl_process.stdout.readline()
+            if line:
+                repl_output_queue.put(line)
+                print(f"REPL output: {line.strip()}")
+                if "REPL ready" in line or "Press 'Enter'" in line or "legend" in line.lower():
+                    repl_ready.set()
+
+    output_thread = threading.Thread(target=read_output, daemon=True)
+    output_thread.start()
+
+    print("Waiting for REPL to be ready...")
+    wait_start = time.time()
+    max_wait = 30  # seconds
+
+    if not repl_ready.wait(timeout=max_wait):
+        print("WARNING: REPL startup may not be complete yet. Sending test command...")
+        try:
+            repl_process.stdin.write("\n")
+            repl_process.stdin.flush()
+            time.sleep(5)
+            if not repl_ready.is_set():
+                print("WARNING: REPL still not ready after additional wait time.")
+            else:
+                print("REPL is now ready to accept commands.")
+        except Exception as e:
+            print(f"Error checking REPL readiness: {e}")
+    else:
+        print("REPL is ready to accept commands.")
+
+    try:
+        print("Sending test command to verify REPL is responsive...")
+        repl_process.stdin.write("help\n")
+        repl_process.stdin.flush()
+
+        verification_timeout = time.time() + 10
+        while time.time() < verification_timeout:
+            try:
+                output = repl_output_queue.get(timeout=0.5)
+                if output and ("Available commands" in output or "help" in output):
+                    print("REPL verified as responsive!")
+                    break
+            except queue.Empty:
+                continue
+
+        time.sleep(2)
+    except Exception as e:
+        print(f"Error sending verification command: {e}")
+
+    return repl_process
+
+
+def send_to_repl(command, timeout=10):
+    """Send a command to the running REPL process."""
+    global repl_process, repl_ready
+
+    if repl_process is None or repl_process.poll() is not None:
+        print("Starting REPL...")
+        start_repl()
+
+    if not repl_ready.is_set():
+        print("Waiting for REPL to be ready before sending command...")
+        if not repl_ready.wait(timeout=30):
+            print("WARNING: REPL may not be fully ready, but attempting to send command anyway.")
+
+    print(f"Sending to REPL: {command}")
+
+    try:
+        try:
+            while True:
+                repl_output_queue.get_nowait()
+        except queue.Empty:
+            pass
+
+        repl_process.stdin.write(command + "\n")
+        repl_process.stdin.flush()
+
+        print("Waiting for REPL response...")
+        wait_start = time.time()
+        max_wait = 20  # Increased timeout for complex queries
+
+        time.sleep(1)
+
+        output = []
+        consecutive_empty_count = 0
+        max_consecutive_empty = 3  # Wait for this many consecutive empty reads before concluding
+
+        if command.startswith("#>"):
+            max_wait = 60  # Much longer timeout for Pure expressions with debug output
+            max_consecutive_empty = 8  # More patience for Pure expressions with verbose output
+
+        while time.time() - wait_start < max_wait:
+            try:
+                line = repl_output_queue.get(timeout=0.5)
+                output.append(line)
+                print(f"Received: {line.strip()}")
+                wait_start = time.time()  # Reset wait timer when we get output
+                consecutive_empty_count = 0  # Reset empty counter
+            except queue.Empty:
+                consecutive_empty_count += 1
+                if output and consecutive_empty_count >= max_consecutive_empty:
+                    print(f"No more output after {consecutive_empty_count} consecutive empty reads")
+                    break
+                continue
+
+        result = "".join(output)
+        if not result:
+            print("No output received from REPL within timeout period.")
+            return "Command sent to REPL (no output within timeout period)"
+
+        print(f"Total output length: {len(result)} characters, {len(output)} lines")
+        return result
+
+    except Exception as e:
+        print(f"Error sending command to REPL: {e}")
+        return f"Error: {str(e)}"
+
+
+def load_csv_to_repl(csv_path, connection_name, table_name):
+    """
+    Load a CSV file into the REPL.
+
+    Args:
+        csv_path: Path to the CSV file
+        connection_name: Name of the connection to use
+        table_name: Name of the table to create
+
+    Returns:
+        dict: The parsed JSON response from the REPL
+    """
+    load_cmd = f"load {csv_path} {connection_name} {table_name}"
+    return send_to_repl(load_cmd)
+
+
+def execute_pure_query(pure_query):
+    """
+    Execute a Pure Relation query in the REPL.
+
+    Args:
+        pure_query: The Pure Relation query to execute
+
+    Returns:
+        dict: The parsed JSON response from the REPL
+    """
+    return send_to_repl(pure_query)
+
+
+def is_repl_running():
+    """
+    Check if the Pure Relation REPL is running.
+
+    Returns:
+        bool: True if the REPL is running, False otherwise
+    """
+    if repl_process is None:
+        start_repl()
+
+    return repl_process != None
Index: runtime/pure/db/type.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/runtime/pure/db/type.py b/runtime/pure/db/type.py
new file mode 100644
--- /dev/null	(date 1745251458344)
+++ b/runtime/pure/db/type.py	(date 1745251458344)
@@ -0,0 +1,21 @@
+from abc import ABC, abstractmethod
+from typing import List
+
+from model.schema import Table, Database
+
+
+class DatabaseType(ABC):
+    def generate_model(self, runtime: str, database: Database) -> str:
+        return f"{self.generate_pure_runtime(runtime, database)}\n\n{self.generate_pure_connection()}\n\n{self.generate_pure_database(database)}"
+
+    @abstractmethod
+    def generate_pure_runtime(self, name: str, database: Database) -> str:
+        pass
+
+    @abstractmethod
+    def generate_pure_connection(self) -> str:
+        pass
+
+    @abstractmethod
+    def generate_pure_database(self, database: Database) -> str:
+        pass
Index: runtime/pure/db/duckdb.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/runtime/pure/db/duckdb.py b/runtime/pure/db/duckdb.py
new file mode 100644
--- /dev/null	(date 1745251458344)
+++ b/runtime/pure/db/duckdb.py	(date 1745251458344)
@@ -0,0 +1,72 @@
+from dataclasses import dataclass
+from datetime import date
+from typing import Type
+
+from model.schema import Table, Database
+from runtime.pure.executionserver.runtime import DatabaseType
+
+@dataclass
+class DuckDBDatabaseType(DatabaseType):
+    path: str
+
+    def generate_pure_runtime(self, name: str, database: Database) -> str:
+        return f"""
+###Runtime
+Runtime {name}
+{{
+  mappings:
+  [
+  ];
+  connections:
+  [
+    {database.name}:
+    [
+      connection: local::DuckDuckConnection
+    ]
+  ];
+}}
+"""
+
+    def generate_pure_connection(self) -> str:
+        return f"""
+###Connection       
+RelationalDatabaseConnection local::DuckDuckConnection
+{{
+  type: DuckDB;
+  specification: DuckDB
+  {{
+    path: '{self.path}';
+  }};
+  auth: Test;
+}}    
+"""
+
+    def generate_pure_database(self, database: Database) -> str:
+        columns = []
+        tables = ""
+        for table in database.tables:
+            for (col, typ) in table.columns.items():
+                columns.append(f"{col} {self._python_type_to_db_type(typ)}")
+            tables += f"""
+  Table {table.table}
+  (
+    {",\n".join(columns)}
+  )
+  
+"""
+        return f"""
+###Relational
+Database {database.name}
+(
+  {tables}
+)
+"""
+
+    def _python_type_to_db_type(self, typ: Type):
+        if typ == str:
+            return "VARCHAR(0)"
+        if typ == int:
+            return "BIGINT"
+        if type == date:
+            return "DATE"
+        raise ValueError(f"Unsupported type {typ}")
\ No newline at end of file
Index: test/dsl/pure_relation_repl_test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/dsl/pure_relation_repl_test.py b/test/dsl/pure_relation_repl_test.py
new file mode 100644
--- /dev/null	(date 1745251458345)
+++ b/test/dsl/pure_relation_repl_test.py	(date 1745251458345)
@@ -0,0 +1,37 @@
+import unittest
+
+from model.schema import Table, Database
+from legendql.ql import LegendQL
+from runtime.pure.repl.repl_utils import is_repl_running, send_to_repl, load_csv_to_repl
+from runtime.pure.repl.runtime import ReplRuntime
+
+
+class TestReplEvaluation(unittest.TestCase):
+
+    def setUp(self):
+        if not is_repl_running():
+            self.skipTest("REPL is not running")
+        load_csv_to_repl("../data/employees.csv", "local::DuckDuckConnection", "employees")
+        load_csv_to_repl("../data/departments.csv", "local::DuckDuckConnection", "departments")
+
+    def tearDown(self):
+        send_to_repl("drop local::DuckDuckConnection employees")
+        send_to_repl("drop local::DuckDuckConnection departments")
+
+    def test_simple_select(self):
+        table = Table("employees", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        runtime = ReplRuntime("local::DuckDuckRuntime")
+
+        data_frame = (LegendQL.from_table(database, table)
+                      .select(lambda r: [r.id, r.departmentId, r.first, r.last])
+                      .bind(runtime))
+        results = data_frame.eval().data()
+        self.assertEqual("""> +--------+--------------+------------+------------+
+|   id   | departmentId |   first    |    last    |
+| BIGINT |    BIGINT    | VARCHAR(0) | VARCHAR(0) |
++--------+--------------+------------+------------+
+|   1    |      1       |    John    |     Doe    |
+|   2    |      1       |    Jane    |     Doe    |
++--------+--------------+------------+------------+
+2 rows -- 4 columns""", results[:results.rfind("columns") + 7])
Index: test/dsl/pure_relation_to_string_test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/dsl/pure_relation_to_string_test.py b/test/dsl/pure_relation_to_string_test.py
new file mode 100644
--- /dev/null	(date 1745251458345)
+++ b/test/dsl/pure_relation_to_string_test.py	(date 1745251458345)
@@ -0,0 +1,72 @@
+import unittest
+
+from dialect.purerelation.dialect import NonExecutablePureRuntime
+from legendql.functions import aggregate
+from model.schema import Table, Database
+from legendql.ql import LegendQL
+
+
+class TestDslToPureRelationDialect(unittest.TestCase):
+
+    def setUp(self):
+        pass
+
+    def test_simple_select(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (LegendQL.from_table(database, table)
+                      .select(lambda e: [e.id, e.departmentId])
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual("#>{local::DuckDuckDatabase.table}#->select(~[id, departmentId])->from(local::DuckDuckRuntime)", pure_relation)
+
+    def test_simple_select_with_filter(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (LegendQL.from_table(database, table)
+                      .select(lambda e: [e.id, e.departmentId])
+                      .filter(lambda e: e.id == 1)
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual("#>{local::DuckDuckDatabase.table}#->select(~[id, departmentId])->filter(e | $e.id==1)->from(local::DuckDuckRuntime)", pure_relation)
+
+    def test_simple_select_with_extend(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (LegendQL.from_table(database, table)
+                      .select(lambda e: [e.id, e.departmentId])
+                      .extend(lambda e: [new_col := e.id + 1])
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual("#>{local::DuckDuckDatabase.table}#->select(~[id, departmentId])->extend(~[new_col:e | $e.id+1])->from(local::DuckDuckRuntime)", pure_relation)
+
+    @unittest.skip("need to support to-string for functions and clean up function metamodel")
+    def test_simple_select_with_groupBy(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (LegendQL.from_table(database, table)
+                      .select(lambda e: [e.id, e.departmentId, e.first, e.last])
+                      .group_by(lambda r: aggregate(
+                                            [r.last],
+                                            [sum_of_id := sum(r.id + 1)],
+                                            having=sum_of_id > 0))
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual(
+            "#>{local::DuckDuckDatabase.table}#->select(~[id, departmentId, first, last])->groupBy(~[last], ~[sum_of_id:r | $r.id+1 : a | $a->sum(), r | $r.sum_of_id > 0])->from(local::DuckDuckRuntime)",
+            pure_relation)
+
+    def test_simple_select_with_limit(self):
+        runtime = NonExecutablePureRuntime("local::DuckDuckRuntime")
+        table = Table("table", {"id": int, "departmentId": int, "first": str, "last": str})
+        database = Database("local::DuckDuckDatabase", [table])
+        data_frame = (LegendQL.from_table(database, table)
+                      .select(lambda e: [e.id, e.departmentId])
+                      .limit(1)
+                      .bind(runtime))
+        pure_relation = data_frame.executable_to_string()
+        self.assertEqual("#>{local::DuckDuckDatabase.table}#->select(~[id, departmentId])->limit(1)->from(local::DuckDuckRuntime)", pure_relation)
Index: test/dsl/pure_relation_exec_server_test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/dsl/pure_relation_exec_server_test.py b/test/dsl/pure_relation_exec_server_test.py
new file mode 100644
--- /dev/null	(date 1745251458345)
+++ b/test/dsl/pure_relation_exec_server_test.py	(date 1745251458345)
@@ -0,0 +1,30 @@
+from model.schema import Table, Database
+from legendql.ql import LegendQL
+from runtime.pure.db.duckdb import DuckDBDatabaseType
+from runtime.pure.executionserver.runtime import ExecutionServerRuntime
+from test.executionserver.test import ExecutionServerTest
+
+table = Table("employees", {"id": int, "departmentId": int, "first": str, "last": str})
+database = Database("local::DuckDuckDatabase", [table])
+
+class TestExecutionServerEvaluation(ExecutionServerTest):
+    @classmethod
+    def setUpClass(cls):
+        ExecutionServerTest.setUpClass()
+        ExecutionServerTest.create_table(table)
+        ExecutionServerTest.load_csv(table, "../data/employees.csv")
+
+    def test_execution_against_execution_server(self):
+        runtime = ExecutionServerRuntime("local::DuckDuckRuntime", DuckDBDatabaseType(ExecutionServerTest.get_duckdb_path()), "http://localhost:6300", database)
+        data_frame = (LegendQL.from_table(database, table)
+                      .select(lambda r: [r.id, r.departmentId, r.first, r.last])
+                      .bind(runtime))
+
+        result = data_frame.eval().data()
+
+        self.assertEqual(result.relation, "#>{local::DuckDuckDatabase.employees}#->select(~[id, departmentId, first, last])->from(local::DuckDuckRuntime)")
+        self.assertEqual(result.sql, 'select "employees_0".id as "id", "employees_0".departmentId as "departmentId", "employees_0".first as "first", "employees_0".last as "last" from employees as "employees_0"')
+        self.assertEqual(",".join(result.header), "id,departmentId,first,last")
+        self.assertEqual(len(result.rows), 2)
+        self.assertEqual(",".join(map(lambda r: str(r), result.rows[0])), "1,1, John, Doe")
+        self.assertEqual(",".join(map(lambda r: str(r), result.rows[1])), "2,1, Jane, Doe")
Index: .gitignore
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.gitignore b/.gitignore
new file mode 100644
--- /dev/null	(date 1745259817334)
+++ b/.gitignore	(date 1745259817334)
@@ -0,0 +1,1 @@
+
Index: README.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/README.md b/README.md
new file mode 100644
--- /dev/null	(date 1745259807645)
+++ b/README.md	(date 1745259807645)
@@ -0,0 +1,3 @@
+# LegendQL
+
+A Python metamodel that represents a Query to be executed in the context of a Runtime, thereby creating a DataFrame.
\ No newline at end of file
Index: test/data/departments.csv
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/data/departments.csv b/test/data/departments.csv
new file mode 100644
--- /dev/null	(date 1745251458344)
+++ b/test/data/departments.csv	(date 1745251458344)
@@ -0,0 +1,2 @@
+id, name
+1, WidgetFactory
Index: test/data/employees.csv
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test/data/employees.csv b/test/data/employees.csv
new file mode 100644
--- /dev/null	(date 1745251458344)
+++ b/test/data/employees.csv	(date 1745251458344)
@@ -0,0 +1,3 @@
+id, departmentId, first, last
+1, 1, John, Doe
+2, 1, Jane, Doe
Index: requirements.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/requirements.txt b/requirements.txt
new file mode 100644
--- /dev/null	(date 1745251458343)
+++ b/requirements.txt	(date 1745251458343)
@@ -0,0 +1,9 @@
+duckdb~=1.2.2
+requests~=2.32.3
+
+pandas~=2.2.3
+polars~=1.27.1
+numpy~=2.2.4
+pyarrow~=19.0.1
+fastavro~=1.10.0
+openpyxl~=3.1.2
\ No newline at end of file
Index: LICENSE
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/LICENSE b/LICENSE
new file mode 100644
--- /dev/null	(date 1744143484036)
+++ b/LICENSE	(date 1744143484036)
@@ -0,0 +1,201 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
Index: .idea/.gitignore
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/.gitignore b/.idea/.gitignore
new file mode 100644
--- /dev/null	(date 1745251458341)
+++ b/.idea/.gitignore	(date 1745251458341)
@@ -0,0 +1,8 @@
+# Default ignored files
+/shelf/
+/workspace.xml
+# Editor-based HTTP Client requests
+/httpRequests/
+# Datasource local storage ignored files
+/dataSources/
+/dataSources.local.xml
Index: .idea/legendql.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/legendql.iml b/.idea/legendql.iml
new file mode 100644
--- /dev/null	(date 1745251458341)
+++ b/.idea/legendql.iml	(date 1745251458341)
@@ -0,0 +1,12 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<module type="PYTHON_MODULE" version="4">
+  <component name="NewModuleRootManager">
+    <content url="file://$MODULE_DIR$">
+      <excludeFolder url="file://$MODULE_DIR$/.venv" />
+      <excludeFolder url="file://$MODULE_DIR$/.venv3126" />
+      <excludeFolder url="file://$MODULE_DIR$/.venv1" />
+    </content>
+    <orderEntry type="jdk" jdkName="Python 3.12 (legendql)" jdkType="Python SDK" />
+    <orderEntry type="sourceFolder" forTests="false" />
+  </component>
+</module>
\ No newline at end of file
Index: .idea/vcs.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/vcs.xml b/.idea/vcs.xml
new file mode 100644
--- /dev/null	(date 1745259636502)
+++ b/.idea/vcs.xml	(date 1745259636502)
@@ -0,0 +1,6 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="VcsDirectoryMappings">
+    <mapping directory="" vcs="Git" />
+  </component>
+</project>
\ No newline at end of file
Index: .idea/.name
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/.name b/.idea/.name
new file mode 100644
--- /dev/null	(date 1745251458341)
+++ b/.idea/.name	(date 1745251458341)
@@ -0,0 +1,1 @@
+legendql
\ No newline at end of file
Index: .idea/modules.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/modules.xml b/.idea/modules.xml
new file mode 100644
--- /dev/null	(date 1745251458341)
+++ b/.idea/modules.xml	(date 1745251458341)
@@ -0,0 +1,8 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="ProjectModuleManager">
+    <modules>
+      <module fileurl="file://$PROJECT_DIR$/.idea/legendql.iml" filepath="$PROJECT_DIR$/.idea/legendql.iml" />
+    </modules>
+  </component>
+</project>
\ No newline at end of file
Index: model/metamodel.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/model/metamodel.py b/model/metamodel.py
new file mode 100644
--- /dev/null	(date 1745251458343)
+++ b/model/metamodel.py	(date 1745251458343)
@@ -0,0 +1,682 @@
+from __future__ import annotations
+from abc import ABC, abstractmethod
+from datetime import date
+from typing import List, Optional
+from dataclasses import dataclass
+
+class Literal[T](ABC):
+    @abstractmethod
+    def value(self) -> T:
+        pass
+
+    @abstractmethod
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        pass
+
+@dataclass
+class IntegerLiteral(Literal):
+    val: int
+    def __init__(self, val):
+        self.val = val
+
+    def value(self) -> int:
+        return self.val
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_integer_literal(self, parameter)
+
+@dataclass
+class StringLiteral(Literal):
+    val: str
+    def __init__(self, val):
+        self.val = val
+
+    def value(self) -> str:
+        return self.val
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_string_literal(self, parameter)
+
+@dataclass
+class DateLiteral(Literal):
+    val: date
+    def __init__(self, val):
+        self.val = val
+
+    def value(self) -> date:
+        return self.val
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_date_literal(self, parameter)
+
+@dataclass
+class BooleanLiteral(Literal):
+    val: bool
+    def __init__(self, val):
+        self.val = val
+
+    def value(self) -> bool:
+        return self.val
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_boolean_literal(self, parameter)
+
+class Function(ABC):
+    @abstractmethod
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        pass
+
+@dataclass
+class CountFunction(Function):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_count_function(self, parameter)
+
+@dataclass
+class AverageFunction(Function):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_average_function(self, parameter)
+
+@dataclass
+class ModuloFunction(Function):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_modulo_function(self, parameter)
+
+@dataclass
+class ExponentFunction(Function):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_exponent_function(self, parameter)
+
+class Expression(ABC):
+    @abstractmethod
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        pass
+
+class Operator(ABC):
+    @abstractmethod
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        pass
+
+class UnaryOperator(Operator, ABC):
+    pass
+
+@dataclass
+class NotUnaryOperator(UnaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_not_unary_operator(self, parameter)
+
+class BinaryOperator(Operator, ABC):
+    pass
+
+@dataclass
+class EqualsBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_equals_binary_operator(self, parameter)
+
+@dataclass
+class NotEqualsBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_not_equals_binary_operator(self, parameter)
+
+@dataclass
+class GreaterThanBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_greater_than_binary_operator(self, parameter)
+
+@dataclass
+class GreaterThanEqualsBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_greater_than_equals_operator(self, parameter)
+
+@dataclass
+class LessThanBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_less_than_binary_operator(self, parameter)
+
+@dataclass
+class LessThanEqualsBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_less_than_equals_binary_operator(self, parameter)
+
+@dataclass
+class InBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_in_binary_operator(self, parameter)
+
+@dataclass
+class NotInBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_not_in_binary_operator(self, parameter)
+
+@dataclass
+class IsBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_is_binary_operator(self, parameter)
+
+@dataclass
+class IsNotBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_is_not_binary_operator(self, parameter)
+
+@dataclass
+class AndBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_and_binary_operator(self, parameter)
+
+@dataclass
+class OrBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_or_binary_operator(self, parameter)
+
+@dataclass
+class AddBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_add_binary_operator(self, parameter)
+
+@dataclass
+class MultiplyBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_multiply_binary_operator(self, parameter)
+
+@dataclass
+class SubtractBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_subtract_binary_operator(self, parameter)
+
+@dataclass
+class DivideBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_divide_binary_operator(self, parameter)
+
+@dataclass
+class BitwiseAndBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_bitwise_and_binary_operator(self, parameter)
+
+@dataclass
+class BitwiseOrBinaryOperator(BinaryOperator):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_bitwise_or_binary_operator(self, parameter)
+
+@dataclass
+class OperandExpression(Expression):
+    expression: Expression
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_operand_expression(self, parameter)
+
+@dataclass
+class UnaryExpression(Expression):
+    operator: UnaryOperator
+    expression: OperandExpression
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_unary_expression(self, parameter)
+
+@dataclass
+class BinaryExpression(Expression):
+    left: OperandExpression
+    right: OperandExpression
+    operator: BinaryOperator
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_binary_expression(self, parameter)
+
+@dataclass
+class LiteralExpression(Expression):
+    literal: Literal
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_literal_expression(self, parameter)
+
+@dataclass
+class AliasExpression(Expression, ABC):
+    alias: str = None
+
+@dataclass
+class VariableAliasExpression(AliasExpression):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_variable_alias_expression(self, parameter)
+
+@dataclass
+class ColumnReferenceExpression(Expression):
+    name: str
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_column_reference_expression(self, parameter)
+
+@dataclass
+class ColumnAliasExpression(AliasExpression):
+    reference: ColumnReferenceExpression = None
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_column_alias_expression(self, parameter)
+
+@dataclass
+class ComputedColumnAliasExpression(AliasExpression):
+    expression: Optional[Expression] = None
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_computed_column_alias_expression(self, parameter)
+
+@dataclass
+class IfExpression(Expression):
+    test: Expression
+    body: Expression
+    orelse: Expression
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_if_expression(self, parameter)
+
+class OrderType(Expression, ABC):
+    pass
+
+@dataclass
+class AscendingOrderType(OrderType):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_ascending_order_type(self, parameter)
+
+@dataclass
+class DescendingOrderType(OrderType):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_descending_order_type(self, parameter)
+
+@dataclass
+class OrderByExpression(Expression):
+    direction: OrderType
+    expression: Expression
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_order_by_expression(self, parameter)
+
+@dataclass
+class FunctionExpression(Expression):
+    function: Function
+    parameters: List[Expression]
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_function_expression(self, parameter)
+
+@dataclass
+class MapReduceExpression(Expression):
+    map_expression: Expression
+    reduce_expression: Expression
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_map_reduce_expression(self, parameter)
+
+@dataclass
+class LambdaExpression(Expression):
+    parameters: List[str]
+    expression: Expression
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_lambda_expression(self, parameter)
+
+class Clause(ABC):
+    pass
+
+@dataclass
+class RenameClause(Clause):
+    columnAliases: List[ColumnAliasExpression]
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_rename_clause(self, parameter)
+
+
+@dataclass
+class FilterClause(Clause):
+    expression: Expression
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_filter_clause(self, parameter)
+
+@dataclass
+class SelectionClause(Clause):
+    expressions: List[Expression]
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_selection_clause(self, parameter)
+
+@dataclass
+class ExtendClause(Clause):
+    expressions: List[Expression]
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_extend_clause(self, parameter)
+
+@dataclass
+class GroupByClause(Clause):
+    expression: Expression
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_group_by_clause(self, parameter)
+
+@dataclass
+class GroupByExpression(Expression):
+    selections: List[Expression]
+    expressions: List[Expression]
+    having: Expression = None
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_group_by_expression(self, parameter)
+
+@dataclass
+class DistinctClause(Clause):
+    expressions: List[Expression]
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_distinct_clause(self, parameter)
+
+@dataclass
+class OrderByClause(Clause):
+    ordering: List[OrderType]
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_order_by_clause(self, parameter)
+
+@dataclass
+class LimitClause(Clause):
+    value: IntegerLiteral
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_limit_clause(self, parameter)
+
+@dataclass
+class FromClause(Clause):
+    database: str
+    table: str
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_from_clause(self, parameter)
+
+class JoinType(ABC):
+    @abstractmethod
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        pass
+
+@dataclass
+class InnerJoinType(JoinType):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_inner_join_type(self, parameter)
+
+@dataclass
+class LeftJoinType(JoinType):
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_left_join_type(self, parameter)
+
+@dataclass
+class JoinExpression(Expression):
+    on: Expression
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_join_expression(self, parameter)
+
+@dataclass
+class JoinClause(Clause):
+    from_clause: FromClause
+    join_type: JoinType
+    on_clause: JoinExpression
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_join_clause(self, parameter)
+
+@dataclass
+class OffsetClause(Clause):
+    value: IntegerLiteral
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_offset_clause(self, parameter)
+
+class Runtime(ABC):
+    @abstractmethod
+    def eval[T](self, clauses: List[Clause]) -> T:
+        pass
+
+    @abstractmethod
+    def executable_to_string(self, clauses: List[Clause]) -> str:
+        pass
+
+    def visit[P, T](self, visitor: ExecutionVisitor, parameter: P) -> T:
+        return visitor.visit_runtime(self, parameter)
+
+@dataclass
+class DataFrame[T](ABC):
+    runtime: Runtime
+    clauses: List[Clause]
+    results: T = None
+
+    def eval(self) -> DataFrame:
+        self.results = self.runtime.eval(self.clauses)
+        return self
+
+    def data(self):
+        return self.results
+
+    def executable_to_string(self) -> str:
+        return self.runtime.executable_to_string(self.clauses)
+
+class ExecutionVisitor(ABC):
+    @abstractmethod
+    def visit_runtime[P, T, R: Runtime](self, val: R, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_from_clause[P, T](self, val: FromClause, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_integer_literal[P, T](self, val: IntegerLiteral, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_string_literal[P, T](self, val: StringLiteral, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_date_literal[P, T](self, val: DateLiteral, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_boolean_literal[P, T](self, val: BooleanLiteral, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_operand_expression[P, T](self, val: OperandExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_not_unary_operator[P, T](self, val: NotUnaryOperator, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_equals_binary_operator[P, T](self, val: EqualsBinaryOperator, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_not_equals_binary_operator[P, T](self, val: NotEqualsBinaryOperator, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_greater_than_binary_operator[P, T](self, val: GreaterThanBinaryOperator, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_greater_than_equals_operator[P, T](self, val: GreaterThanEqualsBinaryOperator, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_less_than_binary_operator[P, T](self, val: LessThanBinaryOperator, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_less_than_equals_binary_operator[P, T](self, val: LessThanEqualsBinaryOperator, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_and_binary_operator[P, T](self, val: AndBinaryOperator, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_or_binary_operator[P, T](self, val: OrBinaryOperator, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_add_binary_operator[P, T](self, val: AddBinaryOperator, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_multiply_binary_operator[P, T](self, val: MultiplyBinaryOperator, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_subtract_binary_operator[P, T](self, val: SubtractBinaryOperator, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_divide_binary_operator[P, T](self, val: DivideBinaryOperator, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_literal_expression[P, T](self, val: LiteralExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_unary_expression[P, T](self, val: UnaryExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_binary_expression[P, T](self, val: BinaryExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_variable_alias_expression[P, T](self, val: VariableAliasExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_computed_column_alias_expression[P, T](self, val: ComputedColumnAliasExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_column_alias_expression[P, T](self, val: ColumnAliasExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_function_expression[P, T](self, val: FunctionExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_map_reduce_expression[P, T](self, val: MapReduceExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_lambda_expression[P, T](self, val: LambdaExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_count_function[P, T](self, val: CountFunction, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_average_function[P, T](self, val: AverageFunction, parameter: P) -> T:
+        raise NotImplementedError()
+
+    def visit_modulo_function[P, T](self, val: ModuloFunction, parameter: P) -> T:
+        raise NotImplementedError()
+
+    def visit_exponent_function[P, T](self, val: ExponentFunction, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_filter_clause[P, T](self, val: FilterClause, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_selection_clause[P, T](self, val: SelectionClause, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_extend_clause[P, T](self, val: ExtendClause, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_group_by_clause[P, T](self, val: GroupByClause, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_group_by_expression[P, T](self, val: GroupByExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_distinct_clause[P, T](self, val: DistinctClause, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_order_by_clause[P, T](self, val: OrderByClause, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_limit_clause[P, T](self, val: LimitClause, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_join_expression[P, T](self, val: JoinExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_join_clause[P, T](self, val: JoinClause, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_inner_join_type[P, T](self, val: InnerJoinType, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_left_join_type[P, T](self, val: LeftJoinType, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_column_reference_expression[P, T](self, val: ColumnReferenceExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_if_expression[P, T](self, val: IfExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_order_by_expression[P, T](self, val: OrderByExpression, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_ascending_order_type[P, T](self, val: AscendingOrderType, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_descending_order_type[P, T](self, val: DescendingOrderType, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_rename_clause[P, T](self, val: RenameClause, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_offset_clause[P, T](self, val: OffsetClause, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_in_binary_operator[P, T](self, self1, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_not_in_binary_operator[P, T](self, self1, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_is_binary_operator[P, T](self, self1, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_is_not_binary_operator[P, T](self, self1, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_bitwise_and_binary_operator[P, T](self, self1, parameter: P) -> T:
+        raise NotImplementedError()
+
+    @abstractmethod
+    def visit_bitwise_or_binary_operator[P, T](self, self1, parameter: P) -> T:
+        raise NotImplementedError()
Index: dialect/purerelation/dialect.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dialect/purerelation/dialect.py b/dialect/purerelation/dialect.py
new file mode 100644
--- /dev/null	(date 1745251458342)
+++ b/dialect/purerelation/dialect.py	(date 1745251458342)
@@ -0,0 +1,215 @@
+from abc import ABC
+from dataclasses import dataclass
+from typing import Set, List
+
+from model.metamodel import ExecutionVisitor, JoinClause, LimitClause, DistinctClause, GroupByClause, ExtendClause, \
+    SelectionClause, FilterClause, FunctionExpression, LiteralExpression, BinaryExpression, \
+    UnaryExpression, OperandExpression, BooleanLiteral, StringLiteral, IntegerLiteral, Runtime, \
+    OrBinaryOperator, AndBinaryOperator, LessThanEqualsBinaryOperator, LessThanBinaryOperator, \
+    GreaterThanEqualsBinaryOperator, GreaterThanBinaryOperator, NotEqualsBinaryOperator, EqualsBinaryOperator, \
+    NotUnaryOperator, InnerJoinType, LeftJoinType, ColumnAliasExpression, \
+    CountFunction, JoinExpression, Clause, FromClause, AddBinaryOperator, \
+    MultiplyBinaryOperator, SubtractBinaryOperator, DivideBinaryOperator, OffsetClause, RenameClause, \
+    OrderByExpression, IfExpression, ColumnReferenceExpression, DateLiteral, GroupByExpression, \
+    ComputedColumnAliasExpression, VariableAliasExpression, MapReduceExpression, LambdaExpression, AverageFunction, \
+    AscendingOrderType, DescendingOrderType, OrderByClause, ModuloFunction, ExponentFunction
+
+
+@dataclass
+class PureRuntime(Runtime, ABC):
+    name: str
+
+    def executable_to_string(self, clauses: List[Clause]) -> str:
+        visitor = PureRelationExpressionVisitor(self)
+        return "->".join(map(lambda clause: clause.visit(visitor, ""), clauses)) + self.visit(visitor, "")
+
+class NonExecutablePureRuntime(PureRuntime):
+    def eval(self, clauses: List[Clause]) -> str:
+        raise NotImplementedError()
+
+@dataclass
+class PureRelationExpressionVisitor(ExecutionVisitor):
+    runtime: PureRuntime
+
+    def visit_runtime(self, val: PureRuntime, parameter: str) -> str:
+        return "->from(" + val.name + ")"
+
+    def visit_from_clause(self, val: FromClause, parameter: str) -> str:
+        return  "#>{" + val.database + "." + val.table + "}#"
+
+    def visit_integer_literal(self, val: IntegerLiteral, parameter: str) -> str:
+        return str(val.value())
+
+    def visit_string_literal(self, val: StringLiteral, parameter: str) -> str:
+        return "'" + val.value() + "'"
+
+    def visit_boolean_literal(self, val: BooleanLiteral, parameter: str) -> str:
+        return str(val.value())
+
+    def visit_operand_expression(self, val: OperandExpression, parameter: str) -> str:
+        return val.expression.visit(self, "")
+    
+    def visit_unary_expression(self, val: UnaryExpression, parameter: str) -> str:
+        return val.operator.visit(self, "") + val.expression.visit(self, "")
+
+    def visit_binary_expression(self, val: BinaryExpression, parameter: str) -> str:
+        return val.left.visit(self, "") + val.operator.visit(self, "") + val.right.visit(self, "")
+    
+    def visit_not_unary_operator(self, val: NotUnaryOperator, parameter: str) -> str:
+        return "!"
+
+    def visit_equals_binary_operator(self, val: EqualsBinaryOperator, parameter: str) -> str:
+        return "=="
+
+    def visit_not_equals_binary_operator(self, val: NotEqualsBinaryOperator, parameter: str) -> str:
+        return "!="
+
+    def visit_greater_than_binary_operator(self, val: GreaterThanBinaryOperator, parameter: str) -> str:
+        return ">"
+
+    def visit_greater_than_equals_operator(self, val: GreaterThanEqualsBinaryOperator, parameter: str) -> str:
+        return ">="
+
+    def visit_less_than_binary_operator(self, val: LessThanBinaryOperator, parameter: str) -> str:
+        return "<"
+
+    def visit_less_than_equals_binary_operator(self, val: LessThanEqualsBinaryOperator, parameter: str) -> str:
+        return "<="
+
+    def visit_and_binary_operator(self, val: AndBinaryOperator, parameter: str) -> str:
+        return "and"
+
+    def visit_or_binary_operator(self, val: OrBinaryOperator, parameter: str) -> str:
+        return "or"
+
+    def visit_add_binary_operator(self, val: AddBinaryOperator, parameter: str) -> str:
+        return "+"
+
+    def visit_multiply_binary_operator(self, val: MultiplyBinaryOperator, parameter: str) -> str:
+        return "*"
+
+    def visit_subtract_binary_operator(self, val: SubtractBinaryOperator, parameter: str) -> str:
+        return "-"
+
+    def visit_divide_binary_operator(self, val: DivideBinaryOperator, parameter: str) -> str:
+        return "/"
+    
+    def visit_literal_expression(self, val: LiteralExpression, parameter: str) -> str:
+        return val.literal.visit(self, "")
+
+    def visit_variable_alias_expression(self, val: VariableAliasExpression, parameter: str) -> str:
+        return "$" + val.alias
+
+    def visit_computed_column_alias_expression(self, val: ComputedColumnAliasExpression, parameter: str) -> str:
+        return val.alias + ":" + val.expression.visit(self, "")
+
+    def visit_column_alias_expression(self, val: ColumnAliasExpression, parameter: str) -> str:
+        return "$" + val.alias + "." + val.reference.visit(self, "")
+
+    def visit_function_expression(self, val: FunctionExpression, parameter: str) -> str:
+        #TODO: AJH: this probably isn't right
+        parameters = list(map(lambda expr: expr.visit(self, ""), val.parameters))
+        function_string = val.function.visit(self, ",".join(parameters[1:]))
+        return parameters[0] + function_string
+
+    def visit_map_reduce_expression(self, val: MapReduceExpression, parameter: str) -> str:
+        return val.map_expression.visit(self, "") + " : " + val.reduce_expression.visit(self, "")
+
+    def visit_lambda_expression(self, val: LambdaExpression, parameter: str) -> str:
+        return ", ".join(val.parameters) + " | " + val.expression.visit(self, "")
+
+    def visit_count_function(self, val: CountFunction, parameter: str) -> str:
+        return "->count()"
+
+    def visit_average_function(self, val: AverageFunction, parameter: str) -> str:
+        return "->avg()"
+
+    def visit_modulo_function[P, T](self, val: ModuloFunction, parameter: P) -> T:
+        return f"->mod({parameter})"
+
+    def visit_exponent_function[P, T](self, val: ExponentFunction, parameter: P) -> T:
+        return f"->pow({parameter})"
+
+    def visit_filter_clause(self, val: FilterClause, parameter: str) -> str:
+        return "filter(" + val.expression.visit(self, "") + ")"
+
+    def visit_selection_clause(self, val: SelectionClause, parameter: str) -> str:
+        return "select(~[" + ", ".join(map(lambda expr: expr.visit(self, ""), val.expressions)) + "])"
+
+    def visit_extend_clause(self, val: ExtendClause, parameter: str) -> str:
+        return "extend(~[" + ", ".join(map(lambda expr: expr.visit(self, ""), val.expressions)) + "])"
+
+    def visit_group_by_clause(self, val: GroupByClause, parameter: str) -> str:
+        return "groupBy(" + val.expression.visit(self, "") + ")"
+
+    def visit_group_by_expression(self, val: GroupByExpression, parameter: str) -> str:
+        selections = "~[" + ", ".join(map(lambda selection: selection.visit(self, ""), val.selections)) + "]"
+        expressions = "~[" + ", ".join(map(lambda expression: expression.visit(self, ""), val.expressions)) + "]"
+        having = ", " + val.having.visit(self, "") if val.having else ""
+        return selections + ", " + expressions + having
+
+    def visit_distinct_clause(self, val: DistinctClause, parameter: str) -> str:
+        return "distinct(~[" + ", ".join(map(lambda expr: expr.visit(self, ""), val.expressions)) + "])"
+
+    def visit_order_by_clause(self, val: OrderByClause, parameter: str) -> str:
+        return "sort([" + ", ".join(map(lambda expr: expr.visit(self, ""), val.ordering)) + "])"
+
+    def visit_limit_clause(self, val: LimitClause, parameter: str) -> str:
+        return "limit(" + val.value.visit(self, "") + ")"
+
+    def visit_join_expression(self, val: JoinExpression, parameter: str) -> str:
+        return "{" + val.on.visit(self, "") + "}"
+
+    def visit_join_clause(self, val: JoinClause, parameter: str) -> str:
+        return "join(" + val.from_clause.visit(self, "")  + ", " + val.join_type.visit(self, "") + ", " + val.on_clause.visit(self, "") + ")"
+
+    def visit_inner_join_type(self, val: InnerJoinType, parameter: str) -> str:
+        return "JoinKind.INNER"
+
+    def visit_left_join_type(self, val: LeftJoinType, parameter: str) -> str:
+        return "JoinKind.LEFT"
+
+    def visit_date_literal(self, val: DateLiteral, parameter: str) -> str:
+        return f"%{val.val.isoformat()}"
+
+    def visit_column_reference_expression(self, val: ColumnReferenceExpression, parameter: str) -> str:
+        return val.name
+
+    def visit_if_expression(self, val: IfExpression, parameter: str) -> str:
+        return f"if({val.test.visit(self, parameter)}, | {val.body.visit(self, parameter)}, | {val.orelse.visit(self, parameter)})"
+
+    def visit_order_by_expression(self, val: OrderByExpression, parameter: str) -> str:
+        return f"~{val.expression.visit(self, parameter)}->{val.direction.visit(self, parameter)}()"
+
+    def visit_ascending_order_type(self, val: AscendingOrderType, parameter: str) -> str:
+        return "ascending"
+
+    def visit_descending_order_type(self, val: DescendingOrderType, parameter: str) -> str:
+        return "descending"
+
+    def visit_rename_clause(self, val: RenameClause, parameter: str) -> str:
+        renames = []
+        for columnAlias in val.columnAliases:
+            renames.append(f"rename(~{columnAlias.reference.visit(self, parameter)}, ~{columnAlias.alias})")
+        return "->".join(renames)
+
+    def visit_offset_clause(self, val: OffsetClause, parameter: str) -> str:
+        return f"drop({val.value.visit(self, parameter)})"
+
+    def visit_in_binary_operator(self, self1, parameter: str) -> str:
+        raise NotImplementedError()
+
+    def visit_not_in_binary_operator(self, self1, parameter: str) -> str:
+        raise NotImplementedError()
+
+    def visit_is_binary_operator(self, self1, parameter: str) -> str:
+        raise NotImplementedError()
+
+    def visit_is_not_binary_operator(self, self1, parameter: str) -> str:
+        raise NotImplementedError()
+
+    def visit_bitwise_and_binary_operator(self, self1, parameter: str) -> str:
+        raise NotImplementedError()
+
+    def visit_bitwise_or_binary_operator(self, self1, parameter: str) -> str:
+        raise NotImplementedError()
\ No newline at end of file
diff --git a/test/simple/__init__.py b/test/simple/__init__.py
new file mode 100644
diff --git a/test/parser/__init__.py b/test/parser/__init__.py
new file mode 100644
diff --git a/test/duckdb/__init__.py b/test/duckdb/__init__.py
new file mode 100644
diff --git a/test/executionserver/__init__.py b/test/executionserver/__init__.py
new file mode 100644
diff --git a/runtime/pure/executionserver/__init__.py b/runtime/pure/executionserver/__init__.py
new file mode 100644
diff --git a/runtime/pure/repl/__init__.py b/runtime/pure/repl/__init__.py
new file mode 100644
diff --git a/runtime/pure/db/__init__.py b/runtime/pure/db/__init__.py
new file mode 100644
diff --git a/test/dsl/__init__.py b/test/dsl/__init__.py
new file mode 100644
diff --git a/test/__init__.py b/test/__init__.py
new file mode 100644
diff --git a/model/__init__.py b/model/__init__.py
new file mode 100644
diff --git a/dialect/__init__.py b/dialect/__init__.py
new file mode 100644
diff --git a/dialect/purerelation/__init__.py b/dialect/purerelation/__init__.py
new file mode 100644
diff --git a/runtime/__init__.py b/runtime/__init__.py
new file mode 100644
diff --git a/runtime/pure/__init__.py b/runtime/pure/__init__.py
new file mode 100644
